## About Kubernetes [[{101,02_doc_has.keypoint]]
- Kubernetes gets in charge of managing (up to thousands of!!!) containers in
  a pool of "node workers". Management takes care of:
  - distribution of application(container) images.
  - storage for apps.
  - networking (including load balancing)
  - Application life-cycle (launch, scale, upgrade, rollback, ...)
[[}]]

## WARN! Kubernetes is for you!  [[{]]
  ============================
- You plan to have a running infrastructure for years,
  and you know in advance that you will need to automate
  lot of RESTfull deployments in an standard way.

- You want to bet for a technology that is well known
  by software industry, so you can hire people that already
  are familiar with it.

- Distributed systems are complex are is better to skip,
  but you can not avoid them. Then k8s is the most familiar
  approach.

- Your company has different isolated teams with random knowledge
  and skills, from finances, to science, to marketing, .....
    They all want a common base and future-resistant virtual
  computing infrastructure.
    They all want to share computer resources, and balance
  computation when needed, as well as reuse DevOps knowledge
  for databases, web servers, networking, ...
    You known that at least they will be share common support
  for the Kubernetes cluster.

- Some k8s operator automate all the life-cycle of software
  management for a given database, CMS, ... You want to profit
  from such existing operator and the knowledge and experience
  of the operator developers.
[[}]]

## WARN! Kubernetes is NOT for you! [[{]]
* Whatsapp grew from 0 to 1.000 Million users with no need for
  complex architectures, just Erlang code runing on Erlang machines.

* Kubernetes is designed for microservices architectures.
  Extracted from @[https://pythonspeed.com/articles/dont-need-kubernetes/]
  """...
    MICROSERVICES ARE AN ORGANIZATIONAL SCALING TECHNIQUE: WHEN YOU HAVE
    500 DEVELOPERS WORKING ON ONE LIVE WEBSITE, IT MAKES SENSE TO PAY THE
    COST OF A LARGE-SCALE DISTRIBUTED SYSTEM IF IT MEANS THE DEVELOPER
    TEAMS CAN WORK INDEPENDENTLY. SO YOU GIVE EACH TEAM OF 5 DEVELOPERS A
    SINGLE MICROSERVICE, AND THAT TEAM PRETENDS THE REST OF THE
    MICROSERVICES ARE EXTERNAL SERVICES THEY CAN’T TRUST.

    IF YOU’RE A TEAM OF 5 AND YOU HAVE 20 MICROSERVICES, AND YOU
    DON’T HAVE A VERY COMPELLING NEED FOR A DISTRIBUTED SYSTEM,
    YOU’RE DOING IT WRONG. INSTEAD OF 5 PEOPLE PER SERVICE LIKE THE BIG
    COMPANY HAS, YOU HAVE 0.25 PEOPLE PER SERVICE.
  """

- Kubernetes is more complex to manage than standard Linux boxes:
  Example extracted from Local Persistent Volume documentation:
  """  Because of these constraints, IT’S BEST TO EXCLUDE NODES WITH
    LOCAL VOLUMES FROM AUTOMATIC UPGRADES OR REPAIRS, and in fact some
    cloud providers explicitly mention this as a best practice. """

    Basically most of the benefits of k8s are lost for apps managing
  storage at App level. This is the case with most modern DDBBs and stream
  architectures (PostgreSQL, MySQL, kafka, p2p-alike Ethereum, ....).
   Such apps needs to be running "for always" and the "moving Pods"
  abstractions of K8s is lost.

- As a reference Google, original creator of K8s, launches
  2 BILLION CONTAINERS PER WEEK!!! in its infrastructures.
  This is not excatly a normal IT load.

- Related. Avoid distributed computing and microservices as much as possible.
@[https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing]
                                         BEFORE opting for Microservices (and K8s) ...
                                         ----------------------------------------------
   1.- "The network is reliable"    <··· Get sure network SLA is higher than Service SLA contract.
   2.- "Latency is zero"            <··· Get sure your system can cache data locally
   3.- "Bandwidth is infinite"
   4.- "The network is secure"      <··· Get sure your traffic uses TLS and PKI is in place.
   5.- "Topology doesn't change"
   6.─ "There is one administrator" <··· Get sure this is true after deployment in production
   7.- "Transport cost is zero"     <··· Get sure your system doesn't slow down with REMOTE DATA
   8.- "The network is homogeneous"
[[}]]

##  Kubernetes Failure Stories  [[{]]
A compiled list of links to public failure stories related to
  Kubernetes. Most recent publications on top.
  NOTE: no new update since 2021 (written as of 2023-04)
@[https://k8s.af/]
[[}]]

## Why Coinbase Is Not Using Kubernetes to Run Their Container Workloads [[{]]
@[https://www.infoq.com/news/2020/07/coinbase-kubernetes/]
  - Coinbase uses containers, but they run them in VMs.
    and Odin for deployments.
    """ Adopting Kubernetes adds unnecessary complexity
        to their current deployment pipeline.···
    Managed k8s services from cloud providers, like EKS or GKE,
    are not mature enough yet. """

  - For application service discovery, they use Route53 in
    conjunction with application load balancers and Envoy.
  - They scale their services through auto-scaling groups (ASGs).
  - Odin has all the logic for doing deployments progressively
    using health checks and can even perform rollbacks when needed.

    """"... they don't think Kubernetes is a bad tool ...
    Knative or Fargate are increasing the level of abstraction to
    solve many of these challenges."""
[[}]]

## Real Problems when applying Readiness Probes [[{]]
   (that will not appear when not using Readiness Probes)
(yes, it can be fixed with better readiness probes, but who knows it!)
  @[https://github.com/helm/charts/issues/24537]
  """ The creation of a new replica fails when the database to replicate is
     very large. It seems that the pod is restarted by the readiness probe
     whilst importing the data into the database. As the import and setup
     are not yet finished the next start fails.

     Database creation/import for slave is interrupted because of
     readiness probe restarting the pod. When the pod has restarted it
     complains the postgres folder is corrupted (missing postgres version).

    How to reproduce it:
    Primary with a very large database and add a replica, log follow and
    see the process being interrupted.
  """
  Unfortunately this problem will appear at random depending on storage/CPU speed.
  It can pass all tests and fail in production due to "parallel" tasks slowing down
  storage.  [[}]]

## k8s reality check 2019 [[{qa.DONTS,01_PM.TODO]]
https://enterprisersproject.com/article/2019/11/kubernetes-reality-check-3-takeaways-kubecon
* Kubernetes still feels complex to beginners.  Enterprise
  distributions can help to abstract away some of this complexity by
  making opinionated choices about components and packaging
  cloud-native ecosystem into a more consumable form.
* rapid evolution&change on areas as diverse as service mesh,
  serverless, policy, monitoring, visualization, and more.
* One can argue whether KubeCon is wholly an infrastructure conference.
  But it certainly isn’t an event really aimed at application developers.
* Operating securely is an important user concern
* Rushgrove (Cloud Native Computing Foundation (CNCF) Security special interest group)
  also said that a lot of the interest he sees in that group is in fairly basic topics,
  such as image scanning and general automation.
  Plenty of Kubernetes security work remains to be done.
  * One area concerns default settings that may make the project easier to adopt
    or use but that aren’t best practice from a security perspective.
  * policy management: The Open Policy Agent (OPA) is one tool that aims to simplify
    security policy (like Enarx for Trusted Execution Environments). As the
    Open Policy Agent (OPA) site states, OPA “has been used to policy-enable software
    across several different domains across several layers of the stack:
    container management (Kubernetes), servers (Linux), public cloud infrastructure
    (Terraform), and microservice APIs (Istio, Linkerd, CloudFoundry).”
    ... while the tools are falling into place a lot of the actual policy isn’t
    written yet.
  * images are downloaded from untrusted sources.
  * containers are run as root or "too much" open permissions.
[[}]]


## Real Problems require real Solutions  [[{qa.DONTS]]
* PROBLEM : developers write code that is impossible to deploy because ...
            it's a BBS (Big Bull ...)
            Manytime they can not even execute in their own laptops or explain
            random configuration errors after any small change.
  SOLUTION: "We encapsulate into containers, emulating their own machine!!!!"

* PROBLEM : Now we have 500 containers!!. Everything is ridicously hard to manage.
  SOLUTION: "We solve it using containers orchestrators!!!"

* PROBLEM : Container orchestrators are ridicously complex to manage and they
            are adding another layer of complexity, not to mention the source
            of exotic problems they have become.
            (Kubernetes, anyone?)
  SOLUTION: "We are on it! sure they will surprise us!!!"

* CONCLUSION: REAL WORLD NEEDS REAL SOLUTIONS:
  Force developers to create FAIL-FAST code.
  * https://en.wikipedia.org/wiki/Fail-fast
  * https://whatdidilearn.info/2017/11/19/exceptions-in-elixir.html
    NOTE: Erlang is designed to build massively scalable soft real-time
          systems with requirements on high availability.
          Some of its uses are in telecoms, banking, e-commerce,
          computer telephony and instant messaging. Erlang's runtime system has
          built-in support for concurrency, distribution and fault tolerance.
          Notably is its use at the core of Whatsapp.

  KEY-POINT: REAL PROBLEMS REQUIRE REAL SOLUTIONS

[[}]]



# BOOTSTRAPPING NEW CLUSTERS [[{cluster_admin.bootstrap.101,101,01_PM.low_code]]

## "Kind" k8s on Docker [[{]]
  @[https://kind.sigs.k8s.io/]
  @[https://github.com/kubernetes-sigs/kind/releases] (latest releases)

- Cite from "Cloud_Native_Spring_in_Action_v7.pdf" (Chapter 2.4)
  """... recommended when you have already installed Docker on your dev machine."""
- Kind replaces "k8s server nodes" by "k8s Docker nodes".
  Features:
  - support for multi-node (including HA) clusters
  - easy switch to a different Kubernetes version/release (from source)
  - Support for Linux, macOS and Windows

- PRE_SETUP) Install 'kind' cli tool:
  (More options available at: @[https://kind.sigs.k8s.io/docs/user/quick-start])
  $ KIND_V="v0.11.1"
  $ SRC_URL="https://kind.sigs.k8s.io/dl"
  $ SRC_URL="${SRC_URL}/${KIND_V}/kind-linux-amd64"
  $ curl -Lo ./kind ${SRC_URL}
  $ chmod +x ./kind


### "Kind" Ussage Cheat Sheet
REF: @[https://itnext.io/kubernetes-kind-cheat-shee-2605da77984]

$ editor .bashrc
+ source <(kind completion bash)  # add auto-completion

$ kind create cluster      <·· Create cluster, 1 single command !!!
                               wait (about 2minutes) for "readiness"
$ kind load docker-image \ <·· Import image from local docker registry
       $imgName:$imgVer

$ kind get clusters        <·· Get running clusters
$ kind delete cluster

Advanced Configuration: kind.yaml


* Map cluster-control-plane:80 to host.

(
  cat <<__EOF | kind create cluster
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
 extraPortMappings:
 — containerPort: 80
 hostPort: 80
 protocol: TCP
__EOF
)

Mount current directory into /app@clusters-control-plane

( cat <<__EOF | kind create cluster
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
 extraMounts:
 — hostPath: .
 containerPath: /app
__EOF
)

### Add Local Registry to Kind:

1. Step 1: Create local registry

   ```sh
   $ docker run -d — restart=always \
     -p 127.0.0.1:5000:5000 \
   ```

2. Step 2: Create cluster:
   ```sh
   ( cat <<__EOF | kind create cluster
   kind: Cluster
   apiVersion: kind.x-k8s.io/v1alpha4
   containerdConfigPatches:
   - |-
    [plugins."io.containerd.grpc.v1.cri".registry.mirrors."localhost:5000"]
    endpoint = [“http://cncf-cheat-sheet-registry:5000"]
   nodes:
   - role: control-plane
   __EOF
   )
   ```

3. Step 3: Connect registry with created network
$ docker network connect kind

4. Step 4: Update cluster about new registry

   ```sh
   ( cat <<__EOF | kubectl apply -f -
   apiVersion: v1
   kind: ConfigMap
   metadata:
    name: local-registry-hosting
    namespace: kube-public
   data:
    localRegistryHosting.v1: |
    host: “localhost:5000”
   __EOF
   )

### Multiple Workers (vs default 1 node -control-pane).

   ```sh
   cat <<__EOF | kind create cluster
   kind: Cluster
   apiVersion: kind.x-k8s.io/v1alpha4
   nodes:
   - role: control-plane
   - role: worker
   __EOF
   ```
[[}]]

## k0s [[{]]
@[https://docs.k0sproject.io/]
All-inclusive Kubernetes distribution.
- bootstrap in minutes with no special skills/expertise.
- Any cloud, Bare metal.
- Edge&IoT ready (etcd cluster can be replaced by sqlite)
- CNCF certified.
- straightforward to keep clusters up-to-date and secure.
  Single single 'k0s' binary (zero dependencies) for any Linux.
- Preferred to Kind when targeting production enviroments.
- Allows setups with restricted Internet access (Airgap Install) [[{security.101}]]
@[https://docs.k0sproject.io/v1.23.6+k0s.2/airgap-install/]

### k0s How-To summary:
$ sudo curl -sSLf https://get.k0s.sh | sudo sh  # <·· Install k0s
$ sudo k0s --help
  k0s command
  ┌───┴─────┘
  config suboption
    ┌────┴───────┘
    create      Output default k0s config to STDOUT
    edit        Launch editor to edit k0s configuration
    status      Display dynamic configuration reconciliation status
    validate    Validate k0s configuration
  install        Install k0s on new system.

  airgap         Manage airgap setup (restricted Internet access setup)
  api            Run the controller API
  completion     Generate bash completion.

  ctr           unsupported debug&admin client to "play"
                with containerd daemon

  docs           Generate k0s command documentation
  etcd           Manage etcd cluster
  kubeconfig     Create `kubeconfig` file for a given user
  kubectl        embedded kubectl
  reset          Uninstall
  status         dump k0s status info.

  start/stop     Start/Stop SystemD/... service in host.
  backup/restore Back-Up/restore k0s configuration.  [[{security.101}]]

  sysinfo        Display system information
  token          Manage join tokens (to add new worker nodes to the cluster)
  version        Print the k0s version

  controller/worker "attach"/worker master node

$ sudo k0s install controller --enable-worker
(Wait a few seconds)
$ sudo k0s start
$ sudo k0s status            # check it's running properly
$ sudo k0s kubectl get nodes # alias kubectl="sudo k0s kubectl"

## REMOTE ACCESS
$ sudo cp /var/lib/k0s/pki/admin.conf ~/admin.conf
$ export KUBECONFIG=~/admin.conf  # Now we can access the cluster itself
3. $ editor ~/admin.conf
-   server: localhost
+   server: https://52.10.92.152:6443

$ sudo k0s kubectl get namespaces
  NAME              STATUS   AGE
  default           Active   5m32s
  kube-node-lease   Active   5m34s
  kube-public       Active   5m34s
  kube-system       Active   5m34s
  ^^^^^^^^^^^^^^^   Active   5m34s
  no master node. control plane implemented  as "naked processes".


$ sudo k0s stop
$ sudo k0s reset              # Clean up previous install and reinstall (next line)
$ sudo k0s install \
  controller --enable-worker -c k0s.yaml
$ sudo k0s start

NOTE: We are free to change the config file even while k0s is running, then just
$ sudo k0s stop
$ sudo k0s start

## Scaling the cluster (adding worker nodes -or control planes)
$ k0s token create --role=worker # fetch token used by new server "phone home".
(a KUBECONFIG BASE64-encoded string will be returned)

In the new worker node:
$ sudo curl -sSLf https://get.k0s.sh | sudo sh
On your new worker host, create a text file to store the long join token you just generated. Then go ahead and install the worker with the join token:

sudo k0s install worker --token-file /path/to/token/file
sudo k0s start

Now if you were to go back to kubectl and check for nodes, you'd see the new node in your list, as in:

kubectl get nodes
NAME               STATUS   ROLES    AGE   VERSION
ip-172-31-14-157   Ready    <none>   81s   v1.19.3
ip-172-31-8-33     Ready    <none>   11h   v1.19.3
[[}]]

## K3s (RapsPi/IoT) [[{cluster_admin.embedded,02_doc_has,01_PM.TODO]]
https://k3s.io/
https://www.itprotoday.com/containers/rancher-labs-k3s-shrinks-kubernetes-edge
 i[./k8s_k3s_how_it_works_revised.svg|width=10em]

- highly available, certified k8s distribution designed for
  PRODUCTION WORKLOADS in unattended, resource-constrained,
  remote locations or inside IoT appliances.
- Simplified and Secure:
  K3s is packaged as a single <50MB binary that reduces the
  dependencies and steps needed to install, run and auto-update a
  production Kubernetes cluster.
- Optimized for ARM RaspPi ut to AWS a1.4xlarge 32GiB server.
- 512MB RAM to run.
[[}]]



## Other cluster bootstrap alternatives [[{]]
- Kubespray: Cluster Bootstrap
  @[https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md]
  - Formed by a set of Ansible playbooks to create a production-ready cluster
    (mono/multi-master, single/distributed etcd, Flannel|Calico|Weave|... network...).

- KOPS: Deploy on Cloud:
  @[https://github.com/tldr-pages/tldr/blob/master/pages/common/kops.md]
  @[https://github.com/kubernetes/kops/blob/master/README.md]
[[cluster_admin.bootstrap.101}]]
[[}]]

## k8s UI Control Panels [[{qa.UX,01_PM.low_code,monitoring.application]]
- 'kubectl' command line tool is probably the most powerful tool to manage k8s clusters
  but for newcomers a UI can be simpler to start with.

- 'kube-dashboard' UI is the  official Dashboard. Install it like:
  $ kubectl apply -f \
    https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
  $ kubectl proxy # makes UI available at:
    http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.


- Alternatives to kube-dashboard include:
  └ Lens """...most powerful UI Panel for K8s ...""" acording to some sources.
  · - Features:
  ·   - Electron based.
  ·   - Supports 'Custom Resource Definitions' (CRD) and Helm3 Package Manager.
  ·   - Built-in Terminal (to launch 'kubectl' inside the UI)
  ·
  └ Octant [easy to install and most portable]
  · · homepage shows overview of Deployments/Pods/ReplicaSets/Services/...
  · · It allows to enable port-forwarding on a pod, read logs, modify pods manifest,
  ·   check pod conditions (initialized, ready, containersReady, PodsScheduled, ...)
  · $ octant # ← creates dashboard at http://localhost:7777 by default.
  ·
  └ 'kubenav' [Android and iOS support]
[[}]]

# Kubernetes 101 [[{101]]
- Ussually referred to as k8s ( k + "ubernete" + s )

- k8s orchestrates pools-of-CPUs, local/remote-storage and networks balancing
      load to a pool of N VMs/physical machines.

- Main Cluster Components and architecture incluse:

  * etcd: k8s "brain memory": High availability key/value ddbb used to save the cluster
          metadata, service registrar and discovery. It favors consistency over speed.
          NOTE for UNIX newcomers: In UNIX configuration is placed inside /etc. etcd is
          sort of a distributed /etc for cloud (distributed) machines, offering a central
          configuration place.

## MASTER-NODE: (or cluster of master nodes for HA)
  - manages the cluster.
  - keeps tracks of:
    - DESIRED STATE  (how many replicas, what entry-points, file-system space, ... We want for our app)
    - APPLICATION ESCALATION.
    - ROLLING UPDATES.
  - Raft consensus used in multi-master mode (requires 1,3,5,... masters)

* MASTER-NODE is composed of:
  ├ kube-apiserver: (REST) Wrapper around k8s objects (containers, volumes, network interfaces, pods, ...)
  │ - It keeps listening for orders from management tools ('kubectl' and UI-like tools)
  │
  ├ kube-controller-manager: EMBEDS k8s CORE CONTROL LOOPS!!!
  │ - handles a number of controllers, regulating the state of  cluster,
  │   performing routine tasks (ensures number of replicas for a service, ...
  │   see later for more info)
  │
  ├ kube─scheduler:
  │ - Assigns workloads to nodes.
  │ - tracks host-resource usage.
  │ - tracks total resources available per server and resources allocated to
  │   existing workloads assigned to each server.
  │ - Manages availability, performance and capacity.
  │
  ├ federation-apiserver: (optional) API server for federated clusters.
  ├ federation-controller-manager: (optional) embeds the core control loops shipped with k8s federation.
  │
  └ cloud-controller-manager: (optional, k8s v1.7+)
    └─┬────────────────────┘
      runs controllers interacting with cloud providers (AWS, GPC, Scaleway, Azure, ...)
      Clouds have extra network services not available in standard Linux boxes and this
      controller allows to use them when available. For example, when balancing an HTTP
      RESTfull service k8s will usually delegate in some Linux application (Envoy,
      Nginx, ...), but in AWS it will be able to use AWS ELB. When requesting new storage
      it will be able to sync with CLOUD storage services (AWS EFS, Ceph, ...)


  │CLUSTER│1 ←····→ N│Namespace│ 1 ←·······→ N│"Application"│
   └┬────┘            └──┬────┘                └──┬────────┘
    ├ etcd       "Virt.cluster"                "informal" term
    │         - Can be assigned max mem/CPU   usually referring to a "Service" or Job
    │           /PVC quotas                   (explained later on) containing storage,
    │                                         injected config files, CPU resources,
    │

Can be assigned max Memory/CPU Quotas
    ├ Master-node/s  (kube-apiserver, kube-controller-manager, kube-scheduler, ...)
    │
    └ Pool of node workers: [worker-node 1, worker-node 2, worker-node 3, ...]
                             └┬────────┘
    ┌─────────────────────────┘
    A worker node is composed of:
    ├ kubelet agent: "daemon" monitoring that containers are running as described in (Pod) Specs
    ├ Container runtime  Different compatible implementations exist from different vendors: runc/crun/rkt/...
    ├ kube-proxy: Virtualized the node network. @[#kube-proxy_summary]
    └ Running Pods: [ Pod1, Pod2, Pod3, .... ]
                      └┬─┘
    ┌──────────────────┘
    A POD IS THE MINIMUM UNIT OF EXECUTION TO BE SCHEDULED BY k8s.
    │ - They can be seen as "virtual machines" that can be moved from
    │   a worker-node to a different worker-node if resources are scarce.
    │ - At creation time resource-limits for CPU/memory are defined.
    │   This allows the kube-scheduler@master to opt for a suitable worker-node.
    └ Container 1,  Container 2, <··· OCI "Docker" images in practice.
      └───────────┬─────────────┘
    ┌─────────────┘
    Usually a single container per Pod, but sometime "related" containers
    get also executed. E.g.: A "main" container contains the PostgreSQL
    ddbb and other "closely related" containers in the same pod get in
    charge of schema initialization, cleaning, backups, GraphQL API, ...
      Containers in a Pod are co-located/co-scheduled and they share storage
    and IP network.

      Pods are ephemeral and can be moved from worker-node to [[{02_DOC_HAS.KEYPOINT]]
    worker-node at random.  They can be created in
    "peaks-of-load" and killed when not needed anymore.  If the
    Pod definition uses a network volume, data is persisted.  [[}]]

## Namespace (Virt.Cluster)  [[{101.namespace]]
@[https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/]
@[https://kubernetes.io/docs/tasks/administer-cluster/namespaces/]

  $ kubectl get namespaces
  NAME          STATUS    AGE
  default       Active    1d
  kube-system   Active    1d

  $  kubectl get pods --all-namespaces   <··· by default only pods in active ns is listed

  $ kubectl create namespace namespc01
·
  $ kubectl --namespace=namespc01 run \ ← Run on given N.S.
            nginx --image=nginx

  $ kubectl config set-context \        ← permanently save NS
    $(kubectl config current-context) \ ← $(...) bash syntax
    --namespace=MyFavouriteNS

  $ kubectl config view | \
    grep namespace: # Validate

- See alos: kubens (by Ahmet Alp Balkan)
  script wrapper to easily switch between Kubernetes namespaces.
  $ kubens foo # activate namespace.
  $ kubens -   # back to previous value.
 @[https://github.com/ahmetb/kubectx]
[[}]]

## Deploying an "App"[[{application.101]]
  Deploying an App in practice means:
  0) (Cluster Admin)    : Create Namespace ("Virtual Kubernetes") for team/project.
  1) (Cluster Admin)    : Prepare Volumes (assign physical drivers to k8s PV)

  2) (Developer)        : Create containerized app/s.
  3) (Developer/DevOps) : Create custom configuration (ConfigMaps, SecretMaps)
  4) (Developer/DevOps) : Define Desired State for running app:
                          containers, replicas, image version, service
                          Probably that means declaring a "Service" (RESTfull like apps)
                          a (Batch)Job, StatefulSet.
  5) (Developer/DevOps) : For Services (Remote HTTP RPC like apps) create "Ingress Rules"
                          exposing inmutable HTTP/S Services balancing to internal
                          (moving) Pods.        [[application.101}]]

## LABELS&LABELS-SELECTORS:                                              [[{101.labels]]
- Kubernetes uses namespace to divide the cluster into virtual clusters.
- Inside a given namespace, it uses labels to organize resources in logical "groups".
  A "kubernetes resource" can be tagged with N simultaneous labels to mark its
  role, target-enviroment, billing account, "anything else" ...
  Usually a label key represents a "dimension" axis and the value assigned the "coordinate".

  LABEL SELECTORS: Similar (in concept) to CSS selectors.
  Target to group of objects (Pods, services, ...) by matching labels.


     key=tier                      ┌─ Lecture ─────────────────────┐
        ^                          │ ◦ == Resource: Pod, Volume,   │
  ... ··┤                          │       LoadBalancerIngress,...)│
        │                          └───────────────────────────────┘
        │   ┌···┐ <······························· selector:
 front··┤   · ◦ ·    ◦      ◦      ◦◦    ◦◦◦◦      'tier=front,enviroment=dev'
        │   └···┘
        │   ┌································┐<··· selector:
 cache··┤   · ◦      ◦      ◦◦           ◦◦◦◦·     'tier=cache'
        │   └································┘
        │
 back···┤     ◦      ◦      ◦◦           ◦◦◦◦
        │
        +─────┬──────┬──────┬──────┬──────┬──··> key=environment
              ·      ·      ·      ·      ·
             dev    QA     pre   canary  pro

-  Ex. labels:
   Key                              | Description       | Example
   ================================   ==================  ================
   app.kubernetes.io/name           | app-name          | useronboarding ┐
   app.kubernetes.io/instance       | unique ID         | mysql-abcxzy-1 │  STANDARD
   app.kubernetes.io/version        | app-version       | 5.7.21         ├  (RECOMMENDED)
   app.kubernetes.io/component      |                   | database       │  LABELS
   app.kubernetes.io/part-of        |                   | wordpress      │
   app.kubernetes.io/managed-by     | mgn.tool used     | helm           ┘
   ================================   ==================  ================
   kubernetes.io/arch               |                   |                ┐
   kubernetes.io/os                 |                   |                │  NON-STANDARD BUT
   kubernetes.io/hostname           |                   |                ├  WELL-KNOWN
   beta.kubernetes.io/instance-type |                   |                │  LABELS
   failure-domain.beta.kubernetes.io/region             |                │
   failure-domain.beta.kubernetes.io/zone               |                ┘
   ================================   ==================  ================
   helm.sh/chart                    | chart name/ver    |                <·· required by HELM
   ================================   ==================  ================
   release                          | canary|stable|... | stable         ┐
┌> environment                      | dev|qa|pre|pro    | test           │  CUSTOM  BY
|  tier                             | front|back|mesg...| front          ├  namespace
|  track                            | daily|weekly|...  | daily          ·
|  ...                                                                   ·
└ NOTE: Probably is saffer and beter to use a different namespace for    ·
        each enviroment, allowing for different quotas, team managing
        passwords, ...

-  Ex. LABEL SELECTORS:

  environment=production,tier!=front   ┐ EQUALITY SELECTOR
                        ^              ┘ "AND" condition
                        └·················┘

  all resources with                   ┐
     ( key    == "tier" AND            │ SET-BASED SELECTOR
       values != frontend OR backend ) │ 'IN', 'NOTIN' 'EXISTS'
  AND                                  │
  all resources with (key != "tier")   ┘


  key equal to environment  and        │ SET─BASED SELECTOR
  value equal to production or qa      │

  environment in (production, qa)      │ SET-BASED SELECTOR

  tier notin (frontend, backend)       │ SET-BASED SELECTOR


  'partition'                          │ "all resources including a label
                                         with key 'partition'
                                         (or '!partition') to neglect

- LIST and WATCH operations may specify label selectors to filter the sets
  of objects returned using a query parameter. Ex.:
  $ kubectl get pods -l environment=production,tier=frontend
  $ kubectl get pods -l 'environment in (production),tier in (frontend)'
  $ kubectl get pods -l 'environment in (production, qa)'
  $ kubectl get pods -l 'environment,environment notin (frontend)'
[[101.labels}]]


## ConfigSecrets&ConfigMaps  [[{application.configuration]]
(See also "Sealed Secrets")
  Custom configuration is injected into Pods as either enviroment variables or configuration
  files (mounted as a volume in the Pod).
  Secrets add some extra layer of protection and they are also use by 'kubelet'@working-node
  to download container images from the container registry if needed.

  - To create a ConfigMap:
    .../config1/                      STEP 1) Setup INPUT DATA
        ├─ game.properties
        └─   ui.properties

    $ kubectl create configmap \      STEP 2) Alt 1. CREATE ConfigMap  from config/ files
      config1 --from-file=config1/

    $ kubectl create configmap \      STEP 2) Alt 2). Create ConfigMap as params.
      config1 \
      --from-literal=key1.sub=val1 \
      --from-literal=...

    $ kubectl get configmaps \        STEP 3)  VERIFY ConfigMap created
      config1 -o yaml
      ...
        data:
        game.properties:
          ...
        ui.properties:

    ┌──────────────────────────────┐
    │kind: Pod                     │
    │...                           │
    │spec:                         │
    │ containers:                  │
    │  ─ name: test-container      │
    │    ...                       │
    │    env:                      │   STEP 4) Declare config1 to be injected in Pod.
    │     ─ name: config1          <·· INJECT "config1" ConfigMap into Pod.
    │       valueFrom:             │
    │         configMapKeyRef:     │
    │           name: game-config  <·· Subconfig in ConfigMap
    │           key: enemies.cheat │
    ...

- k8s Built-in Secrets
  - Service Accounts automatically create and attach secrets with API Credentials.
  - k8s automatically creates secrets with credentials granting API access.
  **NOTE: Pods are automatically modified to use them.**

                                       CREATING USER SECRETS ALT 1
  ================================================================
  $ echo -n .... > ./user.txt          <·· STEP 1) Alt 1. As "secret owner" create un-encrypted secrets
  $ echo -n .... > ./pass.txt                      in local machine.
            ^^^
            └──┴─······························ special characters must be '\' escaped.
  $ kubectl create secret  \           <·· STEP 2) Create k8s Secret
     generic **db-user-pass**\
      --from-file=./user.txt \
      --from-file=./pass.txt

  $ shred -n 2 -z -v -u pass.txt       <·· Safe delete secrets on disk
  $ shred -n 2 -z -v -u user.txt           (vs just a 'rm pass.txt')

                                       CREATING USER SECRETS ALT 2
  ================================================================
  $ cat << EOF > secret.yaml           <·· STEP 1) Alt 2.1. As "secret owner" create k8s Secret  base64 encoded
  apiVersion: v1
  kind: Secret
  metadata:
    name: mysecret
  type: Opaque
  data:                                        'base64': standard Unix tool
    user:$(echo -n .. | base64)            <·· $(...) : Bash syntax suggar: Execute '...' command,
    pass:$(echo -n .. | base64)                         then replace command output as effective value.
  EOF

  $ cat << EOF > secret.yaml           <·· STEP 1) Alt 2.1. As "secret owner" create k8s Secret
  apiVersion: v1                                 with stringData
  kind: Secret
  metadata:
    name: mysecret
  type: Opaque
  stringData:
    config.yaml: |-
      apiUrl: "https://.../api/v1"
      user: admin
      pass: 1f2...

  $ kubectl apply -f secret.yaml       <·· STEP 2) Apply secret.
  $ shred -n 2 -z -v -u secret.yaml     <·· Safe delete secrets on disk

  $ kubectl get secrets                <·· STEP 3) VERIFY/CHECK CREATED SECRET
  > NAME           TYPE    DATA  AGE
  > db-user-pass   Opaque  2     51s


  $ kubectl describe secrets/db-user-pass
  Name: **db-user-pass**
  ...
      Data
      ====
      user.txt:    5 bytes
      pass.txt:    12 bytes


- "INJECTING" SECRETS INTO POD'S CONTAINERS: (As Developer/DevOps)

  ALT 1.A:                        │ ALT 1.B:                              │ ALT 2: CONSUME AS ENV.VAR
  Mount file as volume            │ Mount items file as volume            │ (App will read env.vars to fetch
                                  │                                       │  the secrets)
                                  │                                       │
  apiVersion: v1                  │ apiVersion: v1                        │ apiVersion: v1
  kind: Pod                       │ kind: Pod                             │ kind: Pod
  metadata:                       │ metadata:                             │ metadata:
    name: mypod                   │   name: mypod                         │   name: secret-env-pod
  spec:                           │ spec:                                 │ spec:
    containers:                   │   containers:                         │  containers:
    - name: mypod                 │   - name: mypod                       │  - name: mycontainer
      image: redis                │     image: redis                      │   image: redis
      volumeMounts:               │     volumeMounts:                     │   env:
      - name: foo                 │     - name: foo                       │    - name: SECRET_USERNAME
        mountPath: "/etc/foo"     │       mountPath: "/etc/secret"        │      valueFrom:
        readOnly: true            │       readOnly: true                  │      **secretKeyRef:**
    volumes:                      │   volumes:                            │         name:**db-user-pass**
    - name: foo                   │   - name: foo                         │         key: username
      secret:                     │     secret:                           │    - name: SECRET_PASSWORD
        secretName:**db-user-pass**       secretName:**db-user-pass**     │      valueFrom:
        defaultMode: 256          │       items:                          │      **secretKeyRef:**
                      ^           │       - key: username                 │         name:**db-user-pass**
                      ·           │         path: my-group/my-username    │         key: password
                      ·                           ^^^^^^^^^^^^^^^^^^^^
           JSON does NOT support     · username will be seen in container as:
           octal notation.             /etc/foo/my-group/my-username
           256 = 0400                · password secret is not projected
[[application.configuration}]]

## Sealed Secrets  [[{security.secret_mng]]
@[https://github.com/bitnami-labs/sealed-secrets]
Problem: "I can manage all my K8s config in git, except Secrets."
Solution: Encrypt your Secret into a SealedSecret, which is safe to
store - even to a public repository.
* The SealedSecret can be decrypted only by the controller running in the target cluster and
nobody else (not even the original author) is able to obtain the
original Secret from the SealedSecret.

Sealed Secrets is composed of two parts:

    A cluster-side controller / operator
    A client-side utility: kubeseal

* The kubeseal utility uses asymmetric crypto to encrypt secrets that only the controller can decrypt.
* These encrypted secrets are encoded in a SealedSecret resource,
  which you can see as a recipe for creating a secret. Here is how it
  looks:

apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: mysecret
  namespace: mynamespace
spec:
  encryptedData:
    foo: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....

Once unsealed this will produce a secret equivalent to this:

apiVersion: v1
kind: Secret
metadata:
  name: mysecret
  namespace: mynamespace
data:
  foo: bar  # <- base64 encoded "bar"
[[}]]

## Pod scheduling [[{application.Pod]]
* if the affinity rules cannot be met by any node :
  * Required  affinities: pod will stop being scheduling
  * preferred affinities: pod will still be    scheduled

  Required and preferred affinities can be combined.

  If possible, run my pod in an availability zone without other
  pods labelled app=nginx-ingress-controller.
[[}]]

## Pod securityContext [[{security.101]]
securityContext
apiVersion: v1
  kind: Pod
  metadata:
    name: security-context-demo     $ kubectl exec $runningPod -it -- /bin/bash
  spec:                             # id
    securityContext:        <·····  uid=1000 gid=3000 groups=2000º
      runAsUser: 1000               ( 0/0 if sec.ctx left blank)
      runAsGroup: 3000
      fsGroup: 2000        <······· file-system owned/writable by fsGroup GID
                                    (when supported by volume)
    volumes:
    - name: sec-ctx-vol    <······· Volumes will be relabeled with provided
      emptyDir: {}                  seLinuxOptions values
    containers:
    - name: sec-ctx-demo
      ...
      volumeMounts:
      - name: sec-ctx-vol
        mountPath: /data/demo
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:       <········ Provides a subset of 'root' capabilities
          add:
            - "NET_ADMIN"
            - "SYS_TIME"
       seLinuxOptions:
         level: "s0:c123,c456"

  @[https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h]

* SecurityContext holds security configuration that will be applied to a container.
* SecurityContext settings takes precedence over PodSecurityContext.
*  PodSecurityContext holds pod-level security attributes and
   common container settings
[[}]]






# Controllers [[{101.controllers]]
## ReplicaSet Controller:   [[{101.controllers.replicaset]]
  =====================
  - ensure "N" pod replicas are running simultaneously.
  - Most of the times used indirectly by "Deployments" to
    orchestrate pod creation/deletion/updates.
  - 'Job' controller prefered (vs ReplicaSet) for pods terminating on their own.        [[{02_doc_has.comparative]]
    (batch jobs, cleaning tasks, ...)
  - 'DaemonSet' controller prefered for pods providing a machine-level function.
    (monitoring, logging, pods that need to be running before others pods starts).
    KEY-POINT: DaemonSet PODS  LIFETIME == MACHINE LIFETIME                             [[}]]

  ┌─ app1_frontend_repset.yaml  ┐   $ kubectl create -f http:...app1_frontend_repset.yaml
  │ apiVersion: apps/v1         │     replicaset.apps/frontend created
  │ kind: ReplicaSet            │   $ kubectl describe rs/frontend -l app=guestbook
  │                             │     ...
  │                             │     Replicas:  **3 current / 3 desired**
  │                             │     Pod Template: ...
  │ metadata:                   │       Containers:
  │   name: frontend            │        php-redis: ...
  │   labels:                   │       Volumes:              <none>
┌·····  app: guestbook          │     Events:
├·····  tier: frontend          │     1stSeen LastSeen ... Reason Message
· │ spec:                       │      1m     1m       ... SuccessfulCreate  Created pod: frontend-qhloh
· │   replicas: 3               <···· Default to 1
· │   selector:                 <···· Affected Pods.
· │     matchLabels:            │
· │       tier: frontend        │
· │     matchExpressions:       │
· │       - {key: tier, operator: In, values: [frontend]}
· │   template:                 <···· Pod template (nested pod schema, removing
· │     metadata:               │                   apiVersion/kind properties -)
· │       labels:               <···· Needed in pod-template (vs isolated pod)
├·····      app: guestbook      │      .spec.template.metadata.labels must match
└·····      tier: frontend      │      .spec.selector
  │     spec:                   │
  │       restartPolicy: Always <·· default/only allowed value
  │       containers:           │
  │       - name: php-redis     │
  │         image: gcr.io/google_samples/gb-frontend:v3
  │         resources:          <·· Always indicate resources               [[{qa.best_practices]]
  │           requests:         │   to help scheduler / autoscalers place in most suitable work-node
  │             cpu: 100m       │   and estimate capacity. affinities/anti-affinities also provide hints
  │             memory: 100Mi   │   the scheduler.
  │           limits:           <·· Limits are risky. Use them only when absolute sure that process must
┌··········>    cpu: 200m       │   never pass such limits in normal conditions.
· │             memory: 200Mi   │
· │             ephemeral-storage: 1G ← k8s v1.8+                          [[}]]
· │         env:                │   $ kubectl get pods
· │         - name: PARAM1      │   · NAME             READY     STATUS    RESTARTS   AGE
· │           value: VALUE1     │   · frontend-9si5l   1/1       Running   0          1m
· │         ports:              │   · frontend-dnjpy   1/1       Running   0          1m
· │         - containerPort: 80 │   · frontend-qhloh   1/1       Running   0          1m
· └─────────────────────────────┘
└··· - 1 cpu is equivalent to:
       1 (AWS vCPU|GCP Core|Azure vCore|IBM vCPU|Hyperthread bare-metal)
     - 200m = 0.2 cpu
[[101.controllers.replicaset}]]


## **Deployment** [[{101.controllers.deployment,application.101]]

 - An application is "blur" term in Kubernetes. "Deployment" object is the    [[{02_doc_has.keypoint]]
   most similar entity mapping to the intuitive concept of "Application",
   declaring the "desired set of resources" for the life-cycle of an application.
   (An application is also called "solution" using Microsoft nomenclature or
    service -discouraged since "service" is used in k8s with other meaning relating
    to network inmutable view of the app).

 - Deployments adds application lifecycle to ReplicaSet (creation, updates,...)
   e.g: Pod definition changed: Deployment takes care of gradually moving from
        a running (old) ReplicaSet to a new (automatically created) ReplicaSet.
   Deployments also rollback to (old) ReplicaSet if the new one is not stable.
   (or removing old ones otherwise)                                           [[}]]

─ ┌─ app01_deployment.yaml ──────┐
  │ apiVersion: apps/v1beta2     ←  for vers.<1.7.0 use apps/v1beta1
  │ kind: Deployment             │    $ kubectl create -f app01_deployment.yaml
  │ metadata:                    │    $ kubectl get deployments
  │   name: app1-deployment      │    → NAME             DESIRED CURRENT  ...
  │   labels:                    │    · nginx-deployment 3       0
  │     app: app1                │
  │ namespace: testing_dept1     ← Namespace can be used to
  │ spec:                        │
  │   replicas: 3                ← 3 replicated Pods
  │   strategy:                  │
  │    - type : RollingUpdate    ← Alt: -type: Recreate (used when new version -of DDBB
  │      rollingUpdate:          │      node, ... is NOT compatible with old one)
  │        {maxSurge:2,maxUnavailable:25%}
  │   selector:                  │
  │     matchLabels:             │
 ┌│···    app: app1              │
 ·│   template:                  ← pod template.
 ·│     metadata:                │ TIP: 1+ 'PodPreset' can be used to apply common presets
 ·│       labels:                │      for different pods (similar mount points, injected
 └│······   app: nginx           │      env vars, ...)
  │     spec:                    ← template pod spec
  │       containers:            │ change triggers new rollout
  │       - name: nginx          │
  │         image: nginx:1.7.9   │   $ kubectl rollout status deployment/nginx-deployment
  │         ports:               │   → Waiting for rollout to finish: 2/3 new replicas
  │         - containerPort: 80  │   · have been updated...
  │                              │   · deployment "nginx-deployment" successfully rolled out
  │        volumes:              ← Volumes and ports are the main I/O "devices"
  │    ┌·· - name: test-vol      │ to read input and write output for containers.
  │    ·     hostPath:           │
  │    ·       path: /data       ← location on host
  │    ·       type: Directory   ← (optional)
  │    · ┌ - name:  cache-vol    │
  │    · ·   emptyDir: {}        ← Remove volume when pod is deleted
  │    · ·                       │
  │    · ·   volumeMounts:       │
  │    · ·   - mountPath: /cache ← where to mount
  │    · └..   name:cache-vol    ← volume (name) to mount
  │    ·     - mountPath: ...    │
  │    └····   name:test-vol     │

  │         livenessProbe:       ← Best pattern. Other options include grpc|exec|tcp
  │           httpGet:           ← Alt.: exec: { command: [cat, /tmp/healthy ] }
  │             path: /heartbeat │ Optional parameters:
  │             port: 80         │ · timeoutSeconds  : (before timing out), def: 1sec
  │             scheme: HTTP     │ · successThreshold: Minimum consecutive "OK"s after failure to consider "OK".
  │                              │ · failureThreshold: num.of "KOs" before "giving up" (Pod marked Unready)
  │         readinessProbe: ...  ← Useful when containers takes a long time to start.
  │       activeDeadlineSeconds: 70  ← Optional. Use it when also using Init-Pods (See behind)
  │                              │     to prevent them form failing forever.
  │       initContainers:        │
  │       - name: init-from-url1 ← INIT CONTAINERS: 1+ specialized cont. running IN ORDER before normal ones
  │         image: busybox:1.28  │              - They always run to completion, with k8s restarting
  │                              │                them repeatedly until succeed. (if restartPolicy != Never)
  │         command:             │              - Each one must complete successfully before next one start.
  │         - bash               │              - status is returned in .status.initContainerStatuses
  │         - "-c"               │                (vs .status.containerStatuses) (readiness probes do not apply)
  │         - |                  ← Yaml syntax sugar allowing to embed complex scripts,...
  │           set -ex            │
  │           cat << EOF > file1 │
  │           ...                │
  │           EOF                │
  │           ...                │
  │       ─ name: init-from-db   │
  │         image: busybox:1.28  │
  │         command: ['sh', '─c', 'psql ...']
  └──────────────────────────────┘

  $ kubectl get rs                   <·· dump ReplicaSet created by the deployment
  NAME                    DESIRED ...
  app1-deployment-...4211 3
  └──────┬──────┘ └──┬──┘
  deployment-name- pod─tpl-hash

  $ kubectl get pods --show-labels                      <··· Display ALL labels automatically
  → NAME          ... LABELS                                generated for ALL pods
  · app1-..7ci7o ... app=nginx,...,
  · app1-..kzszj ... app=nginx,...,
  · app1-..qqcnn ... app=nginx,...,

  $ kubectl set image deployment/app1-deployment \      <··· Update nginx 1.7.9 → 1.9.1
    nginx=nginx:1.9.1

  $ kubectl rollout history deployment/app1-deployment  <··· Check deployment revisions
  deployments "nginx-deployment"
  R CHANGE-CAUSE
  1 kubectl create -f nginx-deployment.yaml ---record
  2 kubectl set image deployment/nginx-deployment \
                      nginx=nginx:1.9.1
  ...

  $ kubectl rollout undo deployment/app1-deployment \  <··· Rollback to rev. 2
   --to-revision=2

  $ kubectl scale deployment \                         <··· Scale Deployment
    app1-deployment --replicas=10

  $ kubectl autoscale deployment app1-deployment \
    --min=10 --max=15 --cpu-percent=80
[[101.controllers.deployment}]]
[[101.controllers}]]

## Service: (HTTP)ENDPOINT, INMUTABLE VIEW OF an "Application" [[{101.service]]
  HTTP end point can be RESTfull, GraphQL, GRP,...

- Pods and Pod's network (IPs) are mutable. Pods can move to a different working node
  in the k8s pool "at random", changing their internal IP.
- The k8s "Service" provides the INMUTABLE VIEW (as seen by other Pods/Apps) of the moving
  pods with a PERMANENT internal Cluster IP|DNS name by grouping Pods in a "logical set"
  with a NETWORK POLICY to access them.

  NOTE: The "kube-proxy" gets in charge of the "network magic": @[#kube-proxy_summary]

   Time0 ·················> Time1 ·······> Time2 ·············...
     v                        v              v
   client ··> Service ··┐                  client ··> Service ··┐
   request              ·                  request              ·
              Pod@Node1<┘   ┌···· Node1                         ·
                            ·                                   ·
                  Node2     └>Pod@Node2               Pod@Node2<┘

- SERVICE TYPES:
  =============
  · ClusterIP    : (default) internal-to-cluster virtual IP.
                   No IP exposed outside the cluster.

  · ExternalName : Maps apps (Pods services) to an externally
                   visible DNS entry (ex: foo.example.com)

┌ · NodePort     : (L4) Directly Exposes an external Node-IP:Port to internal Pods.
│
├ · LoadBalancer : (L4) Exposes 'Service' externally using cloud provider's load
│                       balancer. NodePort&ClusterIP services are automa. created.
│
└→ Alternatively 'Ingress' (L7) can be used to expose HTTP/S services externally.
   'NodePort'/'LoadBalancer' Services allows for TCP,UDP,PROXY and SCTP(k8s 1.2+)
   - Advanced load balancing (persistent sessions, dynamic weights) are NOT yet
     supported by Ingress rules (20??-??)
   - 'Ingress' allows to expose services based on HTTP HEADER 'host' (virtual hosting),
      HTTP paths/path regex) ,...  not available to 'NodePort'/'LoadBalancer' Services

              ┌Cloud Provider┐
          ┌···│ LoadBalancer │······┐
          ·   └──────────────┘      ·
          v                         v
      ClusterIP          │       NodePort             │      ExternalName
      =========          │       ========             │      ============
                         │                            │
            ┌···>:80     │   :3100             :80    │           ┌·>|node1├·>:80
            ·    │Pod│   │   ┌··>│node├··┐  ┌─>│Pod│  │           ·           │Pod│
  Internal  ·            │   ·           ·  ·         │ External  ·
  Traffic   ·            │ Incomming     v  ·         │ Traffic   ·
  └·······> ClusterIP:80 │ traffic      │NodePort│    │ └·····> ExternalIP:80
            ·            │   ·           ^  ·         │     │   │LoadBalancer│
            ·            │   ·           ·  ·         │     │     ·
            ·    │Pod│   │   └··>│node├··┘  └·>│Pod│  │ Incomming ·            Pod│
            └···>:80     │   :3100          :80       │   traffic └·>│node2├·>:80
                         │   ^                ^       │
                         │   Creates mapping between  │
                         │   node port(3100) and      │
                         │   Pod port (80)            │


  ┌ service.yaml ────────┐   $ kubectl expose deployment/my-nginx # <- alt1:
  │ apiVersion: v1       │   $ kubectl apply -f service.yaml      # <- alt2:
  │ kind: Service        │
  │ metadata:            │
  │   name: Redis        │ ← Must be valid DNS mapping to DNS entry 'redis.namespace01'
  │ spec:                │    (when using strongly recommended k8s DNS add-on).
  │                      │
  │   selector:          │ ← Targeted Pods (logical group of Pods) *1
  │     app: MyApp       │   ← Matching label/s (ussually something like an App or App Sub-service)
  │                      │
  │   clusterIP: 1.1.1.1 │ ← Optional (discouraged). Used to force IP matching existing DNS entry
  │                      │   or hardcoded ("legacy") IPs difficult to reconfigure to new k8s
  │                      │   deployments (by default clusterIP is  auto-assigned).
  │                      │   KEY-POINT: When the service is up, Pods will be injected ENV.VARs like:
  │                      │     REDIS_SERVICE_HOST : 1.1.1.1   REDIS_PORT: ...
  │   ports:             │     REDIS_SERVICE_PORT : 6379      REDIS_PORT_6379_TCP_...
  │   - name: http       │
  │     protocol: TCP    │ ← := TCP* | UDP | HTTP | PROXY | SCTP(k8s 1.2+)
  │     port: 80         │ ← Request to port 80 will be forwarded to Port 9376
  │     targetPort: 9376 │   on "any" pod matching the selector.
  │   - name: https      │
  │     ...              │
  │   sessionAffinity: "ClientIP" ← Optionally set timeout (defaults to 10800)     [[{troubleshooting.network]]
  └──────────────────────┘          sessionAffinityConfig.clientIP.timeoutSeconds  [[}]]

    *1: There is a particular scenario where the selector is empty:
        'Service' pointing to an 'Endpoint' object  ┌ kind: Endpoints ─────────────────┐
         representing an existing external          │ metadata: { name: ddbb01 }       │
         (TCP) service (DDBB, ERP, API REST, ...)   │ subsets:                         │
         in a different namespace, cluster,         │   - addresses: {  ip: 10.0.0.10 }│
         VM or non-k8s controlled IP                │   - ports:     { port: 5432 }    │
                                                    └──────────────────────────────────┘
  NOTE:  (Much more) detailed info available at
  @[https://kubernetes.io/docs/concepts/services-networking/service/]
   (Section: about kube-proxy + iptables +... advanced config settings)

[[101.service}]]

## Ingress Rules/Controller. [[{network.101.ingress]]
WARN: (update 2023-04) The Gateway-API plans to "replace" Ingress with
      a more versatile approach. More info at @[https://gateway-api.sigs.k8s.io/]
      Istio (probably others) plans to integrate with the Gateway-API in a
      near future.
    @[https://tetrate.io/blog/why-the-gateway-api-is-the-unified-future-of-ingress-for-kubernetes-and-service-mesh/]
  - Add an extra L7 indirection on top of Services.

  - Load Balancer      : layer 4, unaware of the actual apps.       [[{02_doc_has.comparative]]
                         persistent session, dynamic weights.
                         (better for non-HTTP like apps)
  - Ingress controllers: layer 7, can use advanced rules based on
                         inbound URL, ..., apply TLS termination    [[{network.TLS,security.TLS}]]
                         removing TLS cert.complexity from Apps
                         (Better for HTTP like apps)                [[}]]

                            Ingress                            ┌············┐
      POST               ||controller│                         · ┌ node1 -┐ ·┌ node2  ┐
      Host: app1.com ·····>····┐  ┌··│···········>│ServiceA├···┤ │        │ ·│        │
                         ||  ┌····┘  │                         └·> │PodA│ │ └> │PodA│ │
      POST               ││  · ·     │                           │        │  │        │
      Host: app2.com ·····>··┘ └·················>│ServiceB├·····> │PodB│ │  │        │
                         ││          │                           │        │  │        │
       external HTTP/s···┴┘          │                           │ │... │ │  │        │
        endpoint       ^               ^        ^            ^   └────────┘  └────────┘
        endpoing       ·               ·        ·            ·
                       └─ Ingress Con-─┘        └ Services───┘  └─ Pool of working nodes ─┘
                          troller:
                        Layer 7 indirection      Layer 4 indirection.
                       Forward to service        Forward FIX IP to Pod's
                       based on                  (moving) IPs
                       HTTP header|URL path|...

- PRE-SETUP) (by cluster admin) an Ingress controller must be in place:
             ingress-(linkerd|istio|nginx...)

  **WARN: Different Ingress controllers operate slightly differently. **

  ┌─ app1_tls_secrets.yml ─────────┐                                                    [[{security.tls]]
  │ apiVersion: v1                 │
  │ kind: Secret                   ← We need to setup secrets with private cert. key
  │ type: kubernetes.io/tls        │ The secret will be "injected" to the ingress controller.
  │ metadata:                      │ (nginx, istio,...)
┌→│   name: secretTLSPrivateKey    │
· │   namespace: default           │
· │ data:                          │
· │   tls.key: base64 encoded key  ← Secret to protect
· │   tls.crt: base64 encoded cert ← Not really secret.  Public certificate.
· └────────────────────────────────┘                                                    [[}]]
· ┌─ app1_ingress.yml ────────────────────┐
· │ apiVersion: networking.k8s.io/v1beta1 │
· │ kind: Ingress                         │
· │ metadata:                             │
· │   name: test-ingress                  │
· │   annotations:                        │
· │     nginx.ingress.kubernetes.io/rewrite-target: / ← Annotation used before IngressClass was added
· │                                       │             to define the underlying controller implementation.
· │                                       │             .ingressClassName can replace it now.
· │ spec:                                 │
· │   tls:                                ← Optional but recomended. Only 443 port supported.
· │     - hosts:                          │
· │         - secure.foo.com              ← values must  match cert CNs
· │                                       │
└·│······ secretName: secretTLSPrivateKey │
  │   rules:                              ← A backend with no rules could be use to expose a single service
  │   - host: reddis.bar.com              ← Optional. filter-out HTTP requests not addressing this host
  │     http:                             │
  │       paths:                          ← If 2+ paths in list match input request, longest match "wins".
  │       - path: /reddis                 │
  │         pathType: Prefix              ← Implementation Specific: delegate matching to IngressClass (nginx,...)
  │                                       │ Exact : Matches case-sensitive URL path exactly
  │         backend:                      │ Prefix: Matches case-sensitive based on request URL prefix
  │           serviceName: reddissrv      ← targeted k8s service
  │           servicePort: 80             ← Need if Service defines 2+ ports
  │                                       │
  │   - host: monit.bar.com               ← Optional. filter-out HTTP requests not matching host
  │     http:                             │
  │       - path: /grafana                │
  │         pathType: Prefix              │
  │         backend:                      │
  │           serviceName: grafanasrv     │ ← targeted k8s service
  │       - path: /prometheus             │
  │         ...                           │
  │   - http:                             ← Default service for non-matching/not-defined host
  │       - path: /                       │
  │         pathType: Prefix              │
  │         backend:                      │
  │           serviceName: httpdsrv       │ ← targeted k8s service
  └───────────────────────────────────────┘
[[network.101.ingress}]]

## DaemonSet [[{application.daemonset,01_PM.TODO]]
- DaemonSet controller ensures "N" Pods are running in a working node.
- typical uses: cluster storage, log collection, monitoring
  - Ex (simple case): one DaemonSet, covering all nodes, would be used
    for each type of daemon. A more complex setup might use multiple DaemonSets
    for a single type of daemon, but with different flags and/or different memory
    and cpu requests for different hardware types.

Ex.  DaemonSet for fluentd-elasticsearch:
  ┌─ app1_logging_daemonset.yaml ────────┐
  │ apiVersion: apps/v1                  │
  │*kind: DaemonSet *                    │
  │ metadata: ...                        │
  │ spec:                                │
  │   selector: ...                      ← Must be a pod-selector
  │   template:                          ← Pod Template RestartPolicy MUST BE 'Always'
  │     metadata: ...                    │ (default if un-specified)
  │     spec:                            │
  │       nodeSelector: ...              ← ALlows to run only on subset of nodes (vs all)
  │       affinity:     ...              ← Will ron only on nodes matching affinity.
  │       terminationGracePeriodSeconds: 30
  │       tolerations:                   │
  │       - key: node-role.kubernetes.io/master
  │         effect: NoSchedule           │
  │       volumes: ...                   │
  │       containers:                    │
  │       - ...                          │
  └──────────────────────────────────────┘

Daemon Pods do respect taints and tolerations,
but they are created with NoExecute tolerations
for the following taints with no tolerationSeconds:
    node.kubernetes.io/not-ready
    node.alpha.kubernetes.io/unreachable

The following NoSchedule taints are respected:
    node.kubernetes.io/memory-pressure
    node.kubernetes.io/disk-pressure

Pods in DaemonSet can also be marked as critical.

- It is possible to run daemon processes using init or  systemd.
  DaemonSet allows to manages them as standard k8s apps.
[[}]]

## Jobs [[{101.jobs,application]]
- (reliably) run 1+ Pod/s to "N" completions, managing Pods that are
  expected to terminate ("batch jobs") (vs Deployments long-running services).

  ┌─ compute_job.yaml ────────┐ Run like:
  │ apiVersion: batch/v1      │   $ kubectl create -f ./compute_job.yaml   ← Run it
  │ kind:  Job                │
  │ metadata:                 │   $ kubectl describe jobs/pi   ← Check status
  │   name: pi                │   (Parallelism, Completions, Events, ...)
  │ spec:                     │
  │   backoofLimit: 10        ← (Optional, def: 6) Fail after N retries
  │   activeDeadlineSeconds   ← (Optional)
  │   template:               ← associated Pod template
  │     spec:                 │
  │       containers:         │   $ pods=$(kubectl get pods --selector=job-name=pi \
  │       - name: pi          │     --output=**jsonpath={.items..metadata.name}**)
  │         image: perl       │
  │         command: [..,..,] │ - Pods are NOT deleted on completion to allow
  │         - python          │   inspecting logs/output/errors.
  │         - -c              │   '$ kubectl get pods -a' will show them.
  │         - "...."          │
  │         restartPolicy: Never
  │   backoffLimit: 4         │
  └───────────────────────────┘

- Parallel Jobs: Run parallel job with a fixed completion-count
  - job is complete when there is 1-successful-pod for each value
     in the range 1 to .spec.completions.
  - pods must coordinate with themselves or external service to determine
    what each should work on.
  - each pod is independently capable of determining whether or not all its peers
    are done, thus the entire Job is done.
  - For Non-parallel job, leave both .spec.completions and .spec.parallelism unset.
  - Actual parallelism (number of pods running at any instant) may be more or less
    than requested parallelism, for a variety of reasons

- ┌─ cronjob.yaml (v 1.8+) ────────────┐   Alternatively:
  │ apiVersion: batch/v1beta1          │   $ kubectl run hello \
  │ kind: CronJob                      │       --schedule="*/1 0 0 0 0"  \
  │ metadata:                          │       --restart=OnFailure \
  │   name: hello                      │       --image=busybox \
  │ spec:                              │       -- /bin/sh -c "date;"
  │   schedule: "*/1 0 0 0 0"          │
  │   jobTemplate:                     │   $ kubectl get cronjob hello <·· Monitor
  │     spec: ...                      │   $ kubectl get jobs --watch  <·· Watch for job Creat.
  └────────────────────────────────────┘

- TODO:
  - https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/
  - https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/
[[101.jobs}]]


[[101}]]

# "APPLICATION" TROUBLESHOOTING [[{troubleshooting.101]]

## POD MONITORING 101 [[{101,troubleshooting.101,monitoring.application.pods,]]
                     [[01_PM.low_code,02_doc_has.diagram.decission_tree,101.kubectl,application]]
  $ kubectl get pods                           # <·· List pods (in default namespace)
  $ kubectl get pod my-pod                     # <·· ensure pod is running
  $ kubectl top pod POD_NAME --containers      #
  $ kubectl logs my-pod (-c my-container) (-f) # <·· Get logs (-f) to follow "tail"
  $ kubectl attach mypod -i -c $container      # <·· attach to running process
                                                     -i: Pass STDIN to container
                                                     -t: STDIN is a TTY.
                                                     -c $cont: Needed in 2+ containers are
                                                               "co-scheduled" in the same Pod
  $ kubectl port-forward my-pod 5000:6000      # <·· local-machine-port:pod-port
  $ kubectl run -it  busybox \                 # <·· Exec. (temp. pod) shell for OCI image
            --image=busybox -- sh              # <·· -t: create tty, -i: Interactive
  $ kubectl exec my-running-pod -it \          # <·· Get a shell inside already-running Pod
           -- /bin/bash                        # <·· -c my-container needed if Pod has 2+ containers
  $ kubectl exec my-pod env                    # <·· Dump all ENV.VARs inside container
                                                     Very useful to see Services that were "UP"
                                                     at container startup.
[[}]]

## Troubleshooting Decission Tree [[{]]
- @[https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/]

  TRICK: On a separate (tmux)window monitor kubectl resources ike:
         $  watch -n 4 "kubectl get pods,services,... -n $namespace"

  $ kubectl get pods
    └─────┬────────┘
  Q: Is there any pending Pod?
  └─────────┬────────────────┘
  ┌─────────┘
  ├ YES >   $ kubectl describe   < Q: Is the cluster full?
  │              pod $pod_name     └────────┬────────────┘
  │           ┌─────────────────────────────┘
  │           ├ NO  >  Q: Are you hitting ResourceQuotaLimits?
  │           │        └────────────────────┬────────────────┘
  │           │       ┌─────────────────────┘
  │           │       ├ NO  >  Q: Are you mounting a PENDING
  │           │       │           PersistentVolumeClaim?
  │           │       │          (kubectl describe pod $pod will show an even
  │           │       │           "pod has unbound immediate PersistenceVolumeClaim")
  │           │       │        └───────────┬────────────────┘
  │           │       │       ┌────────────┘
  │           │       │       ├ NO  >  $ kubectl get pods     Q: Is the Pod assigned
  │           │       │       │            -o wide               to the Node?
  │           │       │       │                               └────┬───────────────┘
  │           │       │       │      ┌─────────────────────────────┘
  │           │       │       │      ├ YES >  There is an issue with the Kubelet
  │           │       │       │      │
  │           │       │       │      └ NO  >  There is an issue with the Scheduler
  │           │       │       │
  │           │       │       └ YES >  Fix the PersistentVolumeClaim
  │           │       │
  │           │       └ YES >  Relax Quota Limits
  │           │
  │           └ YES > Q: Are nodes "OK"  <·$ kubectl get nodes             [[{cluster_admin]]
  │             ┌─────┴───────────────┘      NAME          STATUS   ROLES    ....
  │             ├  NO > Fix Node/s           master-node   Ready    master,control-plane,...
  │             │                            worker-node01 Ready    worker
  │             │                            worker-node02 Ready    worker [[cluster_admin}]]
  │             └ YES > Add worker node      ...           ^^^^^
  │                                          "Node Problem Detector" DaemonSet helps to monitor
  │                                          node's health
  │
  └ NO  >  Q: Are the Pods Running?
           └──────┬───────────────┘
  ┌───────────────┘
  ├ NO  →  $ kubectl logs $pod_name ←    Q: Can you see the logs
  │        ↑                                for the App?
  │        │                             └─────────┬───────────┘
  │        │  ┌────────────────────────────────────┘
  │        │  ├ Yes >  Fix the issue in the App
  │        │  └ NO  >  Q: Did the container died too Quickly?
  │        │           └──────┬─────────────────────────────┘
  │        │      ┌───────────┘
  │        │      ├ NO  >   $ kubectl describe     Q: Is the Pod in status
  │        │      │             pod $pod_name         ImagePullBackOff?
  │        │      │                                └──┬──────────────────┘
  │        │      │  ┌────────────────────────────────┘
  │        │      │  ├ NO  >  Q: Is the Pode Status CrashLoopBackOff?
  │        │      │  │        └─────────┬─────────────────────────┘
  │        │      │  │ ┌────────────────┘
  │        │      │  │ ├ NO >  Q: Is the Pod status RunContainerError?
  │        │      │  │ │       └─────────┬───────────────────────────┘
  │        │      │  │ │ ┌───────────────┘
  │        │      │  │ │ ├ NO  >  Consult StackOverflow
  │        │      │  │ │ │
  │        │      │  │ │ └ YES >  The issue is likely to be with
  │        │      │  │ │          mounting volumes
  │        │      │  │ │
  │        │      │  │ └ YES >  Q: Did you inspect the logs and fixed the crashes?
  │        │      │  │            $ kubectl logs --previous $POD_NAME
  │        │      │  │          (--previous: See logs of chrased pod)
  │        │      │  │          └─────────┬─────────────────────────┘
  │        │      │  │    ┌───────────────┘
  │        │      │  │    ├ NO  >  Fix the app crahses
  │        │      │  │    │
  │        │      │  │    └ YES >  Q: Did you forget the 'CMD' instruction
  │        │      │  │                in the Dockerfile?
  │        │      │  │             └──────────┬──────────────────────────┘
  │        │      │  │       ┌────────────────┘
  │        │      │  │       ├ YES >  Fix the Dockerfile
  │        │      │  │       │
  │        │      │  │       └ NO  >  Q: Is the Pod restarting frequently?
  │        │      │  │                   Cycling between Running and
  │        │      │  │                   CrashLoopBackoff?
  │        │      │  │                └──────────┬────────────────────────┘
  │        │      │  │        ┌──────────────────┘
  │        │      │  │        ├ YES >  Fix the liveness probe
  │        │      │  │        │
  │        │      │  │        └ NO  >  Unknown State
  │        │      │  │
  │        │      │  └ YES >  Q: Is the name of the image correct?
  │        │      │           └────┬─────────────────────────────┘
  │        │      │   ┌────────────┘
  │        │      │   ├ NO  >  Fix the image name
  │        │      │   │
  │        │      │   └ YES >  Q: Is the image tag valid?
  │        │      │               Does it exists?
  │        │      │            └──┬──────────────────────┘
  │        │      │   ┌───────────┘
  │        │      │   ├ NO  >  Fix the tag
  │        │      │   │
  │        │      │   └ YES >  Q: Are you pulling images from a
  │        │      │               private registry?
  │        │      │            └─────────┬────────────────────┘
  │        │      │    ┌─────────────────┘
  │        │      │    ├ NO  >  The Issue could be with CRI|Kubelet
  │        │      │    │
  │        │      │    └ YES >  Configure pulling images from a
  │        │      │             private registry
  │        │      │
  │        │      └ YES >   $ kubectl logs $pod_name --previous
  │        │                  └────────────┬──────────────────┘
  │        └───────────────────────────────┘
  └ YES >  Q: Are ther Pods READY?
           └──┬──────────────────┘
  ┌───────────┘
  ├ NO  >   $ kubectl describe  <  Q: Is the Readiness probe
  │             pod $pod_name         failing?
  │                                └──┬────────────────────┘
  │           ┌───────────────────────┘
  │           ├ YES >  Fix the Readiness Probe
  │           │
  │           └ NO  >  Unknown State
  │
  └ YES >   $ kubectl port-forward      \   <  Q: Can you access the app?  *1
                $pod_name  8080:$pod_port      └────────────┬───────────┘
                                                            │
      *1 TIP: Test with a command similar to   $ wget localhost:8080/...
         if error "... error forwarding port 8080 to pod 1234..."
         is displayed check that pod 1234... is the intended one by
         executing '$ kubectl describe pods' and checking pod number
         is correct. If it isn't, delete and recreate the service.
         try again until to see if '$ wget ...' works. Continue next
         checks otherwise.)                                 │
                                                            │
  ┌─────────────────────────────────────────────────────────┘
  └   Q: Do you have 2+ different deployments with colliding selector names?
         (this can be the case in "complex" Apps composed of N deployments)
         └──────────────────────┬─────────────────────────────────────────┘
  ┌─────────────────────────────┘
  ├ YES: Fix one or more deployments to avoid colliding selectors
  │      For example if two deployments have a selector labels like
  │      'app: myApp' split into 'app: myApp-ddbb' and 'app: myApp-frontend' or
  │      selector 1:     selector 2:
  │      'app: myapp'    'app: myapp'
  │      'layer: ddbb'   'layer: frontend'
  │      update both on Deployment and related services
  │
  ├ NO  →  Q: Is the port exposed by container correct
  │           and listening on 0.0.0.0?
  │           You can check it like: (-c flag optional for 1 container pods)
  │        $ kubectl exec -ti $pod -c $container -- /bin/sh
  │        # netstat -ntlp
  │        └────────────────────┬────────────────────┘
  │       ┌─────────────────────┘
  │       ├ NO  >  Fix the app. It should listen on
  │       │        0.0.0.0.
  │       │        Update the containerPort
  │       │
  │       └ YES >  Unknown State
  │               Try debugging issues in cluster:
  │              $ SELECTOR=""
  │              $ SELECTOR="${SELECTOR},status.phase!=Running"
  │              $ SELECTOR="${SELECTOR},spec.restartPolicy=Always"
  │              $ kubectl get pods --field-selector=${SELECTOR} \  ← Check failing pods in
  │                -n kube-system                                     kube-system namespace

  ↓
  YES
  ↓
  ===========================
  =POD ARE RUNNING CORRECTLY=
  ===========================
   └──────────┬────────────┘
              ↓
  $ kubectl describe   \    <  Q: Can you see a list of endpoints?
    service $SERVICE_NAME      └────────────┬────────────────────┘
  ┌─────────────────────────────────────────┘
  ├ NO  >  Q: Is the Selector matching the right Pod label?
  │        └──┬───────────────────────────────────────────┘
  │     ┌─────┘
  │     ├ NO  > Fix the Service selector to match targeted-Pod labels
  │     │
  │     └ YES > Q: Does the Pod have an IP address assigned?
  │             └──┬───────────────────────────────────────┘
  │           ┌────┘
  │           ├ NO  > There is an issue with
  │           │       the Controller Manager
  │           │
  │           └ YES > There is an issue with the Kubelet
  │
  └ YES >  $ kubectl port-forward    \     Q: Can you visit the app?
               service/$SERVICE_NAME \     └──┬────────────────────┘
               8080:$SERVICE_PORT             │
                                              │
  ┌───────────────────────────────────────────┘
  ├ NO  >  Q: Is the targetPort on the Service
  │           matching the containerPort in the
  │           Pod?
  │        └──┬───────────────────────────────┘
  │     ┌─────┘
  │     ├ NO  > Fix the Service targetPort and
  │     │     > the containerPod
  │     │
  │     └ YES > The issue could be with Kube Proxy
  ↓
 YES
  ↓
  ==============================
  =SERVICE IS RUNNING CORRECTLY=
  ==============================
  ↓
  $ kubectl describe    \     Q: Can you see a list of Backends?
    ingress $INGRESS_NAME     └──┬─────────────────────────────┘
  ┌──────────────────────────────┘
  ├ NO  >  Q: Are the serviceName and servicePort
  │           mathcing the service?
  │        └──┬─────────────────────────────────┘
  │       ┌───┘
  │       ├ NO  > Fix the ingress serviceName and servicePort
  │       │
  │       └ YES > The issue is specific to the Ingress Controller
  │               Consult the docs for your Ingress
  ↓
 YES
  ↓
  **********************************
  *THE INGRESS IS RUNNING CORRECTLY*
  **********************************
  (The app should be working now!!!)
  |
  v                         ┌ NO  >  The issue is likely to be with the
                            │        Infrastructure and how the cluster is
  Q:  Can you visit app  → ─┤        exposed
      from the Internet?    │
                            │       =========
                            └ YES > =**END**=
                                    =========
[[}]]

## Best Patterns [[{101,qa,01_PM.TODO]]
"...Without an indication how much CPU and memory a container needs,
Kubernetes has no other option than to treat all containers equally.
That often produces a very uneven distribution of resource usage.
Asking Kubernetes to schedule containers without resource
specifications is like entering a taxi driven by a blind person..."
"...The fact that we can use Deployments with PersistentVolumes
does not mean that is the best way to run stateful applications..."
[[}]]

## Managing resources: 5 things to remember [[{101,troubleshooting]]
@[https://enterprisersproject.com/article/2020/8/managing-kubernetes-resources-5-things-remember]
1. Use namespaces and resource quotas
   Resource quotas (limits for namespace) allow cluster admins to control overall resource consumption per namespace.
   (compute, memory, and storage).
   For example, we can set CPU limits or memory limits across all pods in a non-terminal state,
2. Use limit ranges (limits at pod or container level)
3. Set network policies including:
   - how pods communicate (or are prevented from communicating) with one another.
     For example, setiing ingress and egress rules traffic to the pods.
4. Don't forget about storage when applicable:
   - Limits for a namespace can be set for PersistentVolumeClaims.
   - Plan up front whether you need persistent storage in the first place,
     such as for a database, and if you do, which volume plugin is most
     appropriate for the workload.
5. Keep things tidy: API objects and monitoring
   - Do and automate housekeeping.
[[}]]

## Virtual clusters (namespace++) [[{troubleshooting]]
@[https://opensource.com/article/22/3/virtual-kubernetes-clusters-new-model-multitenancy]
Improves over namespace and cluster-based multitenancy

* Namespace-based multitenancy problems:
  * team members can't administer global objects such as custom resource definitions (CRDs).
  * constantly adding exceptions to the namespace isolation rules.
    (network policies, ...)
* Cluster-based multitenancy problems:
  * many clusters to manage, which can be a massive headache.

A virtual cluster is a shared Kubernetes cluster that appears to the tenant as a dedicated cluster.

### vcluster (2020): open source implementation of virtual Kubernetes clusters.
* vcluster allows engineers to provision virtual clusters on top of shared Kubernetes clusters.
  running inside  the underlying cluster's regular namespaces.
* An admin can spin up virtual clusters and hand them out to tenants,
* users can alsow spin up virtual clusters themselves inside their namespace.
  with full control inside the virtual cluster but very restricted access
  outside the virtual cluster.
* Behind the scenes, vcluster accomplishes this by running a Kubernetes API server
  and some other components in a pod within the namespace on the host cluster.
  The user sends requests to that virtual cluster API server inside their namespace
  instead of the underlying cluster's API server.
* vclusters are quick to provision and delete.
[[}]]

## Kompose (docker-compose to K8s)  [[{01_PM.low_code]]
@[https://github.com/kubernetes/kompose]
* tool to automatically convert docker-compose applications to Kubernetes.

  $ kompose convert -f docker-compose.yaml  # Convert to k8s resources file/s
  $ kubectl apply -f .
  $ kubectl get po
  NAME                            READY     STATUS              RESTARTS   AGE
  frontend-591253677-5t038        1/1       Running             0          10s
  redis-master-2410703502-9hshf   1/1       Running             0          10s
  redis-slave-4049176185-hr1lr    1/1       Running             0          10s

Alternatively:
  $ kompose up   -f docker-compose.yml # Deploy to Kubernetes
  $ kompose down -f docker-compose.yml # Delete instantiated services/deployments from Kubernetes
[[}]]

## stern [[{troubleshooting,monitoring.application]]
 - display the tail end of logs for containers and multiple pods.
 - the stern project comes from Wercker (acquired by Oracle in 2017).
 - rather than viewing an entire log to see what happened most
   recently, you can use stern to watch a log ....
[[}]]

## kubetail [[{troubleshooting,monitoring.application]]
  Bash script that enables you to aggregate (tail/follow) logs from
  multiple pods into one stream. This is the same as running "kubectl
  logs -f " but for multiple pods.
[[}]]

## Kured reboot daemon [[{01_PM.TODO]]
Kured, an open-source reboot daemon for Kubernetes.
Kured runs as a [DaemonSet][aks-daemonset] and monitors each node for the presence of
a file indicating that a reboot is required.
[[}]]

## Unused secret detector [[{security.secret_mng,troubleshooting]]
https://github.com/tldr-pages/tldr/blob/master/pages/common/k8s-unused-secret-detector.md

* Command line interface tool for detecting unused Kubernetes secrets.
  More information: https://github.com/dtan4/k8s-unused-secret-detector.

  https://github.com/tldr-pages/tldr/blob/master/pages/common/k8sec.md

  Command line interface tool to manage Kubernetes secrets.
  More information: https://github.com/dtan4/k8sec.
[[}]]

## get External IPs of nodes [[{]]
  $ kubectl get nodes -o jsonpath=\
    '{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
[[}]]



[[troubleshooting.101}]]

# HELM Charts [[{application.helm,101,application,]] @[helm_charts_summary]
  Package manager for Kubernetes with versioning, upgrades and rollbacks.


@[https://helm.sh/docs/intro/quickstart/]

  PRE-SETUP) "kubectl" installed locally.

  INITIAL-SETUP)
  - Download from @[https://github.com/helm/helm/releases]
    and add "helm" command to path.

## HELM "DAILY" USSAGE

  $ helm get -h
  $ helm repo add stable \                  <·· Add repo with name "stable"
    https://kubernetes-charts.storage.googleapis.com/
  $ helm repo add brigade \                 <·· Add another repo
    https://brigadecore.github.io/charts
  $ helm search repo stable                 <·· alt 1: search charts available in repo named 'stable'
  > NAME                          CHART     APP         DESCRIPTION
  >                               VERSION   VERSION
  > stable/acs-engine-autoscaler  2.2.2     2.1.1       DEPRECATED Scales worker nodes within agent pools
  > stable/aerospike              0.2.8     v4.5.0.5    A Helm chart for Aerospike in Kubernetes
  > stable/airflow                4.1.0     1.10.4      Airflow is a platform to programmatically autho...
  > ...

  $ helm search hub wordpress               <·· alt 2: search charts in "ARTIFACT HUB" (dozens of different repos).
  > URL                                                 CHART     APP     DESCRIPTION
  >                                                     VERSION   VERSION
  > https://hub.helm.sh/charts/bitnami/wordpress        7.6.7     5.2.4   Web publishing ...
  > https://hub.helm.sh/charts/presslabs/wordpress-...  v0.6.3    v0.6.3  Presslabs WordPress Operator ....
  > ...

  $ helm repo update                          <··  Make sure we get the latest list of charts (sort of "apt update")
  $ helm show values stable/mysql             <··  Show what params can be customized during install
  $ editor custom01.yaml
  { mysql.auth.username: ...  }

┌>$ helm install                  \           <·· Install MySQL (it can take a several minutes)
·        -values custom01.yaml                    <·· (opt) overwrite defaults values
·        --set   a[0].b=v1,b.c=v1                 <·· (opt) overwrite --values and/or defaults
·        --set   name={a, b, c}                   <·· (opt) custom values list
·        --wait                                   <·· (opt)  Waits for: [min. num of Pods in "ready state", PVCs bound]
·        --timeout: 0m20s                         <·· (opt, def: 5m0s)
·        stable/mysql --generate-name
·
·
· $ helm show chart stable/mysql              ← get an idea of the chart features
· $ helm show all   stable/mysql              ← get all information about chart
·
└ Whenever an install is done, a new local-release is created
  allowing to install multiple times into the same cluster.
  Each local-release can be independently managed and upgraded.
  To list local released:
  $ helm ls --all
  NAME             VERSION   UPDATED        STATUS    CHART
  smiling-penguin  1         Wed Sep 28...  DEPLOYED  mysql-0.1.0
  └──────┬──────┴─················································ local-release == running instance of chart
  Available through INMUTABLE local DNS name:
  miling-penguin.default.svc.cluster.local (port 3306)
                 └─────┴································· (or any active namespace)
  $ kubectl get svc -w smiling-penguin      <·· get service  status("INMUTABLE VIEW" of App -moving pods)

  $ helm uninstall smiling-penguin \        <·· Uninstall local-release
  $      --keep-history                     <·· Allows to rollback deleted install
  Removed smiling-penguin
  $ helm status smiling-penguin
  Status: UNINSTALLED                       <·· works only with '--keep-history'
  $ helm rollback smiling-penguin 1         <···┘

@[https://helm.sh/docs/intro/using_helm/]

NOTE: @[https://www.infoworld.com/article/3541608/kubernetes-helm-gets-full-cncf-approval.html]

## Creating new HELM Charts
  @[https://www.youtube.com/watch?v=3GPpm2nZb2s]
  @[https://helm.sh/docs/developing_charts/]

    $ helm create myapp    ← alt 1: Create from helm template
    myapp/             ← template layout
    ├─ charts/         ← complex apps can consists of many "parallel" charts.
    │                    (front-end service/s, backend/s, middleware, cache, ...)
    │
    │                    Alternatively use requirements.yaml like:
    │                    $ cat requirements.yaml
    │                    | dependencies:
    │                    |   - name: apache
    │                    |     version: 1.2.3
    │                    |     repository: http://repo1.com/charts  ← Pre-Setup:
    │                    |   - name: mysql                            $ helm repo add ...
    │                    |     version: 4.5.6
    │                    |     repository: http://repo2.com/charts
    │                    $ helm dependency update  ← Download deps. to charts/
    │
    ├─ Chart.yaml      ← Sort of chart metadata
    │  ▶│apiversion: v1  ┌Change to 0.2.0 then :
    │   │name: my-nginx  │  $ helm upgrade  my-nginx .   ← Upgrade  0.1.0 → 0.2.0
    │   │version: 0.1.0 ←┘  $ helm rollback my-nginx 1   ← Rollback to rev 1 (0.1.0)
    │   │                   $ helm rollback my-nginx 2   ← Rollback to rev 2 (0.2.0)
    │   │appVersion: 1.0-SNAPSHOT (e.g: we can have different type of helm
    │   │                          deployments (versions) with different types of
    │   │                          replica instances for the same web application)
    │   │description: "chart -vs app- description"
    │   │...  ← eubeVersion, maintainers, template engine, deprecated, ...
    │
    ├─ values.yaml     ← Default values for parameters in yaml templates
    │                    (ports, replica counts, max/min, ...) to be replaced as
    │                    {{ .Values.configKey.subConfigKey.param }}.
    │                    They can be overloaded at install/upgrade like:
    │                    $ helm install|upgrade ... --set configKey.subConfigKey.param=...
    └─ templates
       ├─ deployment.yaml  ← Can be created manually like:
       │                     $ kubectl create deploy nginx --image nginx \
       │                       --dry-run -o yaml > deployment.yaml
       │ 
       ├─ service.yaml     ← Can be created manually like:
       │                     $ kubectl expose deploy mn-nginx -port 80   \
       │                       --dry-run -o yaml > service.yaml
       │ 
       ├─ _helpers.tpl     ← files prefixed with '_' do not create k8s output
       │                     Used for reusable pieces of template
       ├─ hpa.yaml
       ├─ ingress.yaml
       ├─ NOTES.txt
       ├─ serviceaccount.yaml
       └─ tests
          └── test-connection.yaml

- DEBUGGING TEMPLATE/s:
  $ helm lint                            ← verify best-practices
  $ helm install --dry-run --debug myApp
  $ helm template --debug

- INSTALLING THE CHART:                 Install from current dir. (vs URL / Repo)
  $ helm install --name my-nginx .   ←  TIP: Monitor deployment in real time in another
                                        console like: $ watch -n 2 "kubectl get all"
  $ helm delete --purge my-nginx     ← Clean-up install

## Helm Operator
@[https://docs.fluxcd.io/projects/helm-operator/en/1.0.0-rc9/references/helmrelease-custom-resource.html]

- operator watching for (Custom Resource) HelmRelease change events (from K8s).
  It reacts by installing or upgrading the named Helm release.
  (See oficial dock for setup/config params)


    apiVersion: helm.fluxcd.io/v1
    kind: HelmRelease              ← Custom resource
    metadata:
      name: rabbit
      namespace: default
    spec:
    · releaseName: rabbitmq        ← Automatically generated if not provided
    · targetNamespace: mq          ← same as HelmRelease project if not provided.
    · timeout: 500                 ← Defaults to 300secs.
    · resetValues: false           ← Reset values on helm upgrade
    · wait: false                  ← true: operator waits for Helm upgrade completion
    · forceUpgrade: false          ← true: force Helm upgrade through delete/recreate
    · chart:                       ← alt 1: chart from helm repository
    ·   repository: https://charts....     ← HTTP/s and also S3 / GCP Storage through extensions
    ·   name: rabbitmq
    ·   version: 3.3.6
    · # chart:                       ← alt 2: chart from git repository: Repo will be cloned
    · #                                       - git pooled every 5 minutes (See oficial doc. to
    · #                                         skip waiting)  *1
    · #
    · #   git: git@github.com:fluxcd/flux-get-started
    · #   ref: master
    · #   path: charts/ghost
    · values:                      ← values to override on source chart
    ·   replicas: 1
    · valuesFrom:                  ← Secrets, config Maps, external sources, ...
    · - configMapKeyRef:
    ·     name: default-values     ← mandatory
    ·     namespace: my-ns         ← (of config. map) defaults to HelmRelease one
    ·     key: values.yaml         ← Key in config map to get values from
    ·                                (defaults to values.yaml)
    ·     optional: false          ← defaults to false (fail-fast),
    ·                                true=> continue if values not found
    · - secretKeyRef:
    ·     name: default-values         ← mandatory
    ·     namespace: my-ns             ← (Of secrets) defautl to HelRelease one
    ·     key: values.yaml             ← Key in the secret to get the values from
    ·                                    (defaults to values.yaml)
    ·     optional: false          ← defaults to false (fail-fast),
    ·                                true=> continue if values not found
    ·  - externalSourceRef:
    ·      url: https://example.com/static/raw/values.yaml
    ·     optional: false          ← defaults to false (fail-fast),
    ·                                true=> continue if values not found
    ·
    · rollback:                    ← What to do if new release fails
        enable: false              ← true : perform rollbacks for releas.
        force: false               ← Rollback through delete/recreate if needed.
        disableHooks: false        ← Prevent hooks from running during rollback.
        timeout: 300               ← Timeout to consider failure install
        wait: false                ← true => wait for min.number of Pods, PVCs, Services
                                             to be ready state before marking release as
                                             successful.

   $ kubectl delete hr/my-release  ← force reinstall Helm release
                                     (upgrade fails, ... )
                                     Helm Operator will receive delete-event and force
                                     purge of Helm release. On next Flux sync, a new Helm Release
                                     object will be created and Helm Operator will install it.


   *1 : Git Authentication how-to:
        - Setup the SSH key with read-only access ("deploy key" in GitHub)
          (one for each accessed repo)

        - Inject each read-only ssh key into Helm Release operator
          by mounting "/etc/fluxd/ssh/ssh_config" into the container
          operator and mounting also (as secrets) each referenced
          priv.key in "/etc/fluxd/ssh/ssh_config"


- About helm-charts pod templates:
@[https://helm.sh/docs/chart_best_practices/pods/]
"...A container image should use a fixed tag or the SHA of the image. It
 should not use the tags latest, head, canary, or other tags that are
 designed to be "floating"..."

## Resources install order [[{101]]
  (Each resource has an associated "Resource"Spec and a "Resource"Status)
     1  Namespace················ split k8s into virtual clusters
     2  NetworkPolicy············ TODO:
     3  ResourceQuota············ TODO:
     4  LimitRange··············· TODO:
     5  PodSecurityPolicy········ TODO:
     6  PodDisruptionBudget······ TODO:
     7  ServiceAccount··········· Used by k8s internal pods (and knative apps?) interacting with k8s apiserver.
     8  Secret··················· Injected as ENV.VARs or ConfigFiles in running Pods
     9  SecretList···············
    10  ConfigMap·················Injected as ENV.VARs or ConfigFiles in running Pods
    11  StorageClass············· Tunning of Storage (ceph|aws|gce|azure|... performance, fs, HD|SSD, ...)
    12  PersistentVolume········· storage I/O resource applying to whole cluster.
    13  PersistentVolumeClaim···· PersistentVolume claimed by Controller or Pod.
    14  CustomResourceDefinition· TODO: Migrate a ThirdPartyResource to CustomResourceDefinition:
        @[https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/migrate-third-party-resource/]
    15  ClusterRole·············· TODO
    16  ClusterRoleList·········· TODO
    17  ClusterRoleBinding······· TODO
    18  ClusterRoleBindingList··· TODO
    19  Role····················· TODO
    20  RoleList················· TODO
    21  RoleBinding·············· TODO
    22  RoleBindingList·········· TODO
    23  Service·················· INMUTABLE VIEW of applications (vs its "moving" Pods IPs/ports)
    24  DaemonSet················ Controller prefered for pods providing a machine-level function.
    25  Pod······················ runnable "unit of work" that can be scheduled (1+ Containers)
    26  ReplicationController···· Deprecated in favor of Deployments configuring a ReplicaSet.
    27  ReplicaSet··············· ensure "N" pod replicas are running simultaneously.
    28  Deployment··············· Adds application lifecycle to ReplicaSet (e.g.: start wtih a
                                  ReplicaSet of 3, gradually move to ReplicaSet of 5, update
                                  with new ReplicaSet with new version, ...)
    29  HorizontalPodAutoscaler·· Autoscales Deployments/StatefulSet to match demand  [[{application.scalability]]
                                  running more Pods.
        @[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/] [[}]]
  A Cluster Autoscaler must also be considered when using pod autoscaler.


    30  StatefulSet··············
    31  Job······················
    32  CronJob··················
    33  Ingress··················
    34  APIService···············
[[101}]]


## Kubeapps App Dashboard [[{]]
@[https://kubeapps.com/]
  - Deploy Apps Internally:
    Browse Helm charts from public or your own private chart repositories
    and deploy them into your cluster.
  - Manage Apps:
    Upgrade, manage and delete the applications that are deployed in your
    Kubernetes cluster.
  - Service Catalog: (Deprecated)
    Browse and provision external services from the Service Catalog and
    available Service Brokers.
[[}]]

Storage [[{ $storage ]]
# Storage 101 [[{storage.101,application,cloud.storage,01_PM.TODO,]]
REF:@[https://www.youtube.com/watch?v=OulmwTYTauI]



          Cluster Admin tasks (PRESETUP)              Developer tasks
       ┌──────────────┴──────────────────┐   ┌──────────┴─────────────┐
     STEP 0)  ¹,² ╶╶╶╶╶╶▷ STEP 1) ³╶╶╶╶╶╶╶╶╶╶╶▷ STEP 2)  ╶╶╶▷ STEP 3)
     cluster admins       cluster admins        App.Dev       App.Dev
     ==============       ==============        =======       =======
     Add storage          Add PV (storage       Add PVC       Link PVC to Volumes
     "hardware" (or       I/O resource                        and Volumes to
     NFS, Cloud,..)       applying to the                     Mount Points@Containers
     partition, format    to whole cluster
     and mount disks to
     nodes using standard OS utilities
     ¹: raw block device support, used by Oracle,... is also provided.
        bypassing OS file-system and gaining in performance.
     ²: K8s is also able to format the block device on demand, avoiding
        manual formating. (any gain?)
     ³: PV can be created:
       - Alt 1: Manually
       - Alt 2: Let a DaemonSet handle the creation.

                                               once underlying
                                               storage has been
                                               assigned to a pod
        ┌···plugin is one of ····←┐              PV is bound to PVC
    ┌───↓──────────────┐    ┌Persistent ┐      one-to-one relation
    │-NFS: path,srvDNS │    │ Volume(PV)│···┐ ┌───────────────────┐   ┌ Container@Pod┐
    │-AWS: EBS|EFS|... │    └───────────┘   · │ Persistent  *1    ←·┐ │              │
    │-Azure:...        │                    · │ Volume            │ · │ - Vol.Mounts │
    │-...              │    ┌ Local ────┐   · │ Claim (PVC)       │ · │   /foot      │
    └ NETWORK ATTACHED ┘  ┌·· Persistent····┤ │                   │ · │              │
      STORAGE             · │Volume(LPV)│   · │  100Gi            │ · │ ┌───────────┐│
                          · └── GA 1.14+┘   └··· Selector         │ · │ │ Volumes:  ││
    ┌──────────────┐      ·          *2     ┌··· StorageClassName │ └···· ─PVC      ││
    │ eNVM,SSD     ·······┘                 · └───────────────────┘   │ │ ─claimName││
    └─ PCI/CPU-BUS ┘          ┌─Storage ─┐  ·                         │ └───────────┘│
  ▶ Local disk/s requires     │ Class(SC)│··┘                         └──────────────┘
    LocalPV(vs PV) since      └──────────┘
    it also affects Pod      (L)PV LIFESPAM: ◁ ─ ─ ─ ─ ─ ─ ─ ─ ─ ▷     VOLUME LIFESPAM:
    allocation scheduling    that of cluster                           that of Pod

  TIP:  Sometimes, it is useful to share one volume for multiple uses in a single pod.
  'volumeMounts.subPath' can be used to specify a sub-path inside the volume (vs root).

   *1  - Similar to how Pods can request/limit CPU and Memory, PVClaims can request
         specific size and access modes (mount once read/write, many-times read-only, ...)
       - PV contains "maxsize", PVC contains "minsize" with PV.maxsize > PVC.minsize

 *2 Local Persistent Volume: [[{]]
  - REF: @[https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumes-ga/]
    Authors: Michelle Au (Google), Matt Schallert (Uber), Celina Ward (Uber)
  - LPV LEVERAGES HIGH PERF./DIRECTLY-ATTACHED DISKS (PCI vs network)
  - Preferred for applications handling data replication themself.
    (software defined storage, replicated databases, blockchains, kafka,
     Cassandra, ...).
    Discourages for other types of apps.PVs takes care of replication.

    hostPath vs LPV:
    Q: "hostPath" already allows to use local disk as a storage for Pods.
       why SHOULD we use Local Persistent Volume instead?
    A: With Local PV, the Scheduler is aware of the Local PV, so on Pod
       re-start, execution will be assigned to the same worker node.
       With hostPath, k8s can re-schedule in a different node, loosing all
       previously stored data.

  - USSAGE STEPS:

     PRE-SETUP Plannification)
     - how many LOCAL disks would each node cluster have?
     - How would they be partitioned?
       The **local static provisioner** provides guidance to help answer
       these questions. It can be used to help manage the Local PV lifecycle
       - create,clean up,reuse - for individual disks.
       @[https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner]
     - WARN: DYNAMIC VOLUME PROVISIONING NOT SUPPORTED. [[{01_PM.TODO.update}]]

       Hint: It’s best to be able to dedicate a full disk to each local
             volume (for IO isolation) and a full partition per-volume (for
             capacity isolation).

     STEP 1) Create StorageClass:
       ┌──────────────────────────────────────────┐
       │ kind: StorageClass                       │
       │ apiVersion: storage.k8s.io/v1            │
       │ metadata:                                │
       │   name: local-storage                    │
       │ provisioner: kubernetes.io/no─provisioner│
       │ volumeBindingMode: WaitForFirstConsumer  ← enable  volume topology-aware scheduling
       └──────────────────────────────────────────┘ Consumer == "Pod requesting the volume"

     STEP 2) the external static provisioner can be configured and run to
             create PVs for all the local disks on your nodes.
       $ kubectl get pv
       NAME              CAPA  ACC. RECLAIM  STATUS    CLAIM STORAGECLASS  REASON AGE
                               MODE POLICY
       local-pv-27c0f084 368Gi RWO  Delete   Available       local-storage         8s
       local-pv-3796b049 368Gi RWO  Delete   Available       local-storage         7s
       local-pv-3ddecaea 368Gi RWO  Delete   Available       local-storage         7s

     STEP 3) Start using PVs in workloads by:
       - Alt 1: creating a PVC and Pod (not show)
       - Alt 2: creating a StatefulSet with volumeClaimTemplates
         ┌ Ex: ───────────────────────────────────┐
         │ apiVersion: apps/v1                    │
         │ kind: StatefulSet                      │
         │ metadata:                              │
         │   name: local-test                     │
         │ spec:                                  │
         │   serviceName: "local-service"         │
         │   replicas: 3                          │
         │   selector: ...                        │
         │   template: ...                        │
         │     spec:                              │
         │       containers:                      │
         │       - name: test-container           │
         │         ...                            │
         │         volumeMounts:                  │
         │ ┌····   - name: local-vol              │
         │ ·         mountPath: /usr/test-pod     │
         │ · volumeClaimTemplates:                │
         │ · - metadata:                          │
         │ └···· name: local-vol                  │
         │     spec:                              │
         │       accessModes: [ "ReadWriteOnce" ] │
         │       storageClassName:"local-storage" │
         │       resources:                       │
         │         requests:                      │
         │           storage: 368Gi               │
         └────────────────────────────────────────┘
          Once the StatefulSet is up and running, the PVCs will be bound:

       $ kubectl get pvc
          NAME                   STATUS VOLUME              CAPACITY ACC.. STORAGECLASS   AGE
          local-vol-local-test-0 Bound  local-pv-27c0f084   368Gi    RWO   local-storage  3m45s
          local-vol-local-test-1 Bound  local-pv-3ddecaea   368Gi    RWO   local-storage  3m40s
          local-vol-local-test-2 Bound  local-pv-3796b049   368Gi    RWO   local-storage  3m36s

     STEP 4) Automatic Clean Up. Ex modifying replicas for stateful set (sts):
       $ kubectl patch sts local-test \     ←  e.g: Reduce Pod replicates 3→2 for StatefulSet
         -p '{"spec":{"replicas":2}}'               associated local-pv-... is not needed anymore.
       statefulset.apps/local-test patched          The external static provisioner will clean up
                                                    the disk and make the PV available for use again.

       $ kubectl delete pvc local-vol-local-test-2
       persistentvolumeclaim "local-vol-local-test-2" deleted

       $ kubectl get pv
       NAME              CAPA  ACC. RECLAIM  STATUS    CLAIM STORAGECLASS  REASON AGE
                               MODE POLICY
       local-pv-27c0f084 368Gi RWO  Delete   Bound      ...-0 local-storage        11m
       local-pv-3796b049 368Gi RWO  Delete   Available        local-storage         7s
       local-pv-3ddecaea 368Gi RWO  Delete   Bound      ...-1 local-storage        19m
                                                        └─┬─┘
                                            default/local-vol-local-test-0/1

    - LPV LIMITATIONS AND CAVEATS:
      - It ties apps to a specific node, making it harder to schedule.
        Those apps   should specify a high priority  so that lower priority pods,
        can be preempted if necessary.
      - Also if the tied-to node|local volume become inaccessible, then the pod
        also becomes inaccessible requiring manual intervention, external controllers
        or operators.
      - If a node becomes unavailable (removed from the cluster or drained), pods using
        local volumes on that node are stuck in "Unknown" or "Pending" state depending
        on whether or not the node was removed gracefully.
        To recover from these interim states:
        STEP 1) the PVC binding the pod to its local volume must be deleted
        STEP 2) the pod must be deleted to forcerescheduled
        (or wait until the node and disk are available again).
        "... We took this into account when building our operator for
         M3DB, which makes changes to the cluster topology when a pod is
         rescheduled such that the new one gracefully streams data from the
         remaining two peers..."
        "...Thanks to the k8s scheduler’s intelligent handling of volume topology,
         M3DB is able to programmatically evenly disperse its replicas across multiple
         local persistent volumes in all available cloud zones, or, in the case of     [cloud]
         on-prem clusters, across all available server racks..."

      - Because of these constraints, IT’S BEST TO EXCLUDE NODES WITH
        LOCAL VOLUMES FROM AUTOMATIC UPGRADES OR REPAIRS, and in fact some
        cloud providers explicitly mention this as a best practice.
        (Basically most of the benefits of k8s are lost for apps managing
         storage at App level. This is the case with most DDBBs and stream
         architectures).
[[}]]

@[https://kubernetes.io/docs/concepts/storage/volume-snapshot-classes/]
- Dynamic Provisioning (when supported by the underlying StorageClass)  [[{cloud.billing]]
  allows to provising storage on-demand                                 [[}]]

- Dynamic volume limits (k8s v1.17+)
@[https://kubernetes.io/docs/concepts/storage/storage-limits/]
  Allowed for Amazon EBS, Google Persistent Disk, Azure Disk, CSI (2023-02)
  - On GCE       : up to 127 volumes per node.
  - On Amazon EBS: M5,C5,R5,T3 and Z1D instance types, up to 25 volumes per Node.
                   Other               instance types, up to 39 volumes per Node.
  - Azure        : up to 64 disks per node.

## StorageClass: [[{storage.tunning]]
Map to different I/O performance level, SLAs, backups and custom policies.

kubernetes.io/... <··· AzureFile,AzureDisk,AWSElasticBlockStore,CephFS,Cinder,
                       FC,FlexVolume,GCEPersistentDisk,iSCSI,NFS,RBD,
                       VsphereVolume,PortworxVolume,Local
aws-ebs.gp2       <··· Custom StorageClass: aws supports io1, gp2, sc1, st1.
                       (and for each one we can tune iopsPerGB, fsType, encryption, ..)
                       can be customized)

- Different Storage Classes allows for different tunning  (fstype, replication-type,
  encryption, magnetic or SSD, ...)

- reclaimPolicy := Delete (default) or Retain  [[{security.backups}]]
[[storage.tunning}]]

[[storage.101}]]

[[ $storage }]]

# Operators [[{101,application.operators,qa,01_PM.TODO]]

* By design Operators can provision, update, and delete
  Kubernetes resources - which means they are in-cluster privileged
  components.
Operator's common misunderstandings, according to Deo:
* OperatorHub.io
* building operators:
  Operator Framework, Kubebuilder

- "Clearing House" for k8s Ops:
@[https://www.datacenterknowledge.com/open-source/aws-google-microsoft-red-hats-new-registry-act-clearing-house-kubernetes-operators]
  - AWS, Google, Microsoft, Red Hat's New Registry to Act as Clearing
    House for Kubernetes Operators.
  - Operators make life easier for Kubernetes users, but they're so
    popular that finding good ones is not easy. Operatorhub.io is an
    attempt to fix that.
[[}]]


# SERVICE MESH COMPARATIVE [[{cloud.sdn,02_doc_has.comparative,01_PM.low_code,application.observability,application.development]]
## istio.io SERVICE MESH
- L7 Software defined network.
- istio forwards "any" request from Pod to Pod (for app-to-app) through a set
  of Envoys (C++) proxies installed on each working node and controlled by istio.
  This allows istio to control the traffic "globally" (at the cost of some extra
  "slow-down" in network communications). It can control routing, rate limiting,
  (distributed microservice traffic) OBSERVABILITY, mTLS, and other capabilities
  with "zero" development costs, apply global network policies, ...
- Network/Applications admins interact with istio through a control pane.
- Works within/outside Kubernetes.

## linkerd SERVICE MESH:
- GO/Rust based.
- Used linkerd2-proxy, faster/lighter than (istio) Envoy.
- Similar to istio.io but looks to be lighter and faster for K8s.
- Only works in Kubernetes.
- Does NOT offer as much options as Istio but is simpler to install.
[[ $networking }]]


# Microservices "APPLICATION" testing [[{qa.testing]]
## Kube-monkey Chaos testing [[{qa.testing.chaos,01_PM.TODO]]
- stress test, breaking stuff at random.
- It works by randomly killing pods in a cluster
  that you specifically designate, and can be fine-tuned
  to operate within specific time windows.

See also:
https://kubernetes.io/blog/2020/01/22/kubeinvaders-gamified-chaos-engineering-tool-for-kubernetes/
[[}]]

## Litmus chaos engineering framework [[{01_PM.TODO]]
https://www.infoq.com/news/2019/05/litmus-chaos-engineering-kube/
Chaos Engineering Kubernetes with the Litmus Framework

Litmus:
- open source chaos engineering framework for Kubernetes
  environments runninG stateful applications
- Litmus can be added to CI/CD pipelines
- designed to catch hard-to-detect bugs in Kubernetes
  that might be missed by unit or integration tests.
- focus on application resilience:
  - pre-existing tests for undesirable behavior such:
    - container crash
    - disk failure
    - or network delay
    - packet loss.
- can also be used to determine if a Kubernetes deployment
  is suitable for stateful workloads.
[[}]]

[[}]]

# Monitoring [[{ $monitoring ]]
## crictl: Debug node [[{monitoring.cluster,01_PM.TODO]]
@[https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/]
[[}]]

## Events in Stackdriver [[{cluster_admin,monitoring,troubleshooting,01_PM.TODO]]
@[href=https://kubernetes.io/docs/tasks/debug-application-cluster/events-stackdriver/]
- Kubernetes events are objects that provide insight into what is
  happening inside a cluster, such as what decisions were made by
  scheduler or why some pods were evicted from the node.

- Since events are API objects, they are stored in the apiserver on
  master. To avoid filling up master's disk, a retention policy is
  enforced: events are removed one hour after the last occurrence. To
  provide longer history and aggregation capabilities, a third party
  solution should be installed to capture events.

- This article describes a solution that exports Kubernetes events to
  Stackdriver Logging, where they can be processed and analyzed.
[[}]]

## Metrics API+Pipeline [[{cluster_admin,monitoring,troubleshooting,01_PM.TODO]]
- Resource usage metrics, such as container CPU and memory usage, are
  available in Kubernetes through the Metrics API. These metrics can be
  either accessed directly by user, for example by using kubectl top
  command, or used by a controller in the cluster, e.g. Horizontal Pod
  Autoscaler, to make decisions.

@[https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/]
@[https://github.com/kubernetes-incubator/metrics-server]

  Extracted from @[https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/]
  """...If running on Minikube, run the following command to enable the metrics-server:
  $ minikube addons enable metrics-server

  ... to see whether the metrics-server is running, or another provider of the resource metrics
  API (metrics.k8s.io), run the following command:

  $ kubectl get apiservices
     output must include a reference to metrics.k8s.io.
     → ...
     → v1beta1.metrics.k8s.io
  """
[[}]]
[[ $monitoring }]]

K8s Extension Tools [[{ $extension_tools ]]

# KubeDB: Production DBs [[{storage.kubedb,application.operators,01_PM.low_code,qa,01_PM.TODO]]
- operators exists for MySQL/PostgreSQL/Redis/... BUT THERE ARE PLENTY OF GAPS.

  KUBEDB ALLOWS TO CREATE CUSTOM KUBERNETES OPERATORS FOR MANAGING DATABASES
  - Running backups, cloning, monitoring, snapshotting, and declaratively
    creating databases are all part of the mix.
  - supported features vary among databases. Ex: clustering is available for
    PostgreSQL but not MySQL.
[[}]]

[[$extension_tools }]]

# Cluster Governance [[{ $governance ]]
## Gatekeeper policy controls [[{security.101,qa.governance,security.standards,security.aaa,security.auditing,01_PM.backlog]]
- The Open Policy Agent project (OPA) provides a way to create policies
  across cloud-native application stacks, from ingress to service-mesh
  components to Kubernetes. More info available at:
  @[../Cloud/cloud_map.html#OPA_Summary]
- OPA DECOUPLES POLICY DECISION-MAKING FROM POLICY ENFORCEMENT.
- Gatekeeper provides a Kubernetes-native way to enforce OPA
  policies on a cluster automatically, and to audit for any events or
  resources violating policy. All this is handled by a relatively new
  mechanism in Kubernetes, admission controller Webhooks, that fire on
  changes to resources. With Gatekeeper, OPA policies can be maintained
  as just another part of your Kubernetes cluster's defined state,
  without needing constant babysitting.
[[}]]
## Teleport [[{security.aaa.2FA,cluster_admin.ssh,security.auditing,01_PM.backlog]]
- implement industry-best practices for SSH and Kubernetes access,
  meet compliance requirements, and have complete visibility into access and behavior.

· Security best practices out-of-the-box.
· Isolate critical infra and enforce 2FA with SSH and Kubernetes.
· Provide role-based access controls (RBAC) using short-lived
certificates and your existing identity management service.
· Log events and record session activity for full auditability.
[[}]]

## Kubecost [[{security.auditing,cloud.billing,troubleshooting,01_PM.backlog]]
- monitor "the dollars" cost of running Kubernetes?

- Kubecost uses real-time Kubernetes metrics, and real-world cost
  information derived from running clusters on the major cloud
  providers, to provide a dashboard view of the monthly cost of each
  cluster deployment. Costs for memory, CPU, GPU, and storage are all
  broken out by Kubernetes component (container, pod, service,
  deployment, etc.).

- Kubecost can also track the costs of “out of cluster” resources,
  such as Amazon S3 buckets, although this is currently limited to AWS.

- Kubecost is free to use if you only need to keep 15 days of logs. For
  more advanced features, pricing starts at $199 per month for
  monitoring 50 nodes.
[[}]]

## Auditing users/admins/others [[{cluster_admin,security.auditing,01_PM.TODO]]
@[https://kubernetes.io/docs/tasks/debug-application-cluster/audit/]
- Kubernetes auditing provides a security-relevant chronological set of
  records documenting the sequence of activities that have affected
  system by individual users, administrators or other components of the
  system. It allows cluster administrator to answer the following
  questions:
  - what happened?
  - when did it happen?
  - who initiated it?
  - on what did it happen?
  - where was it observed?
  - from where was it initiated?
  - to where was it going?
[[}]]



[[$governance }]]

# Cluster Administration [[{ $cluster_admin ]]
## Internal (Pods&Services) DNS [[{101,01_PM.TODO ]]
@[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/]
* Kubernetes creates DNS records for Services and Pods. (no need for IPs anymore)
* Kubelet configures Pods' DNS
* By default, a client Pod's DNS search list includes:
  * Pod's own namespace
  * cluster's default domain.
  Querying in another non-default namespace require explicit mention like:
  'someService.someNameSpace'
* Kubelet can also update Pod's /etc/resolv.conf

  FQDN for Pod:
       172-17-0-3.subdomain.domain01.pod.cluster.local.
                                        └───────────┴─····· Domain name for our cluster
                                    └─┴─··················· K8s resource type
                            └──────┴─······················ Namespace of Pod
                   └───────┴─······························ (optional in Pod's spec.subdomain)
       └─────────┴─········································ Pod IP (or spec.hostname if set)

### Pod spec.dnsPolicy
* "Default"                : The Pod inherits name resolution config from the node that the Pods run on.
* "ClusterFirst"           : Any non-matching DNS query is forwarded to upstream nameserver.
* "ClusterFirstWithHostNet": (for Pods running with hostNetwork only)
* "none": Use dnsConfig in this case.

### Pod's spec.dnsConfig: v1.14+
- it works with any dnsPolicy settings.
- when Pod's dnsPolicy is set to "None", this field is mandatory.
* spec.dnsConfig.nameservers: IP_address{1,3}
  The list will be combined to the base nameservers generated
  from the specified DNS policy.
* spec.dnsConfig.searches: DNS search domain list (up to 6 search domains).
* spec.dnsConfig.options: optional name-value property list
  to be merged with the options generated from the specified DNS policy.

## spec.HostAliases [[{]]
- set /etc/hosts in running Pod.
 @[https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/]
- Useful when when DNS and other options are not applicable.
$ editor myPod.yml
  apiVersion: v1
  kind: Pod
  metadata:
    name: my-pod01
  spec:
    restartPolicy: Never
+   hostAliases:
+   - ip: "127.0.0.1"
+     hostnames:
+     - "foo.local"
+     - "bar.local"
+   - ip: "10.1.2.3"
+     hostnames:
+     - "foo.remote"
+     - "bar.remote"
    containers:
    - name: cat-hosts
      image: busybox:1.28
      command:
      - cat
      args:
      - "/etc/hosts"

$ kubectl apply -f myPod.yaml
$ kubectl exec my-pod01 -- cat /etc/hosts
> # Kubernetes-managed hosts file.
> 127.0.0.1	localhost
> ::1	localhost ip6-localhost ip6-loopback
> fe00::0	ip6-localnet
> fe00::0	ip6-mcastprefix
> fe00::1	ip6-allnodes
> fe00::2	ip6-allrouters
> 10.200.0.5	hostaliases-pod
>
> # Entries added by HostAliases.
> 127.0.0.1	foo.local	bar.local
> 10.1.2.3	foo.remote	bar.remote

* WARN: avoid modifying with internal Pod's scripts.
  /etc/hosts is managed by kubelet and can be overwritten "at random".

  Q: Why does the kubelet manage the hosts file?
  A: kubelet manages it for each container of the Pod to
    prevent the container runtime from modifying the file after the
    containers have already been started.
      Historically, Kubernetes always used Docker Engine as its
    container runtime, and Docker Engine would then modify the /etc/hosts
    file after each container had started.
      Current Kubernetes can use a variety of container runtimes; even
  so, the kubelet manages the hosts file within each container so that
  the outcome is as intended regardless of which container runtime you
  use.
[[}]]



[[}]]

## K8s cluster networking 101 [[{101,cluster_admin,network.101,troubleshooting ]]
- Max Node Lattency:
@[https://stackoverflow.com/questions/46891273/kubernetes-what-is-the-maximum-distance-latency-supported-between-kubernetes-no]
- There is no latency limitation between nodes in kubernetes cluster.
  They are configurable parameters.
  └ For kubelet@worker-node:
    --node-status-update-frequency secs
      Sets how often kubelet posts node status to master.
      Note: be cautious when changing the constant, it must work
      with nodeMonitorGracePeriod in nodecontroller. (default 10s)
  └ For controller-manager on master node:
    --node-monitor-grace-period secs
      Amount of time which we allow running Node to be unresponsive
      before marking it unhealthy. Must be N times more than kubelet's
      'nodeStatusUpdateFrequency', where N means number of retries
      allowed for kubelet to post node status. (default 40s)
    --node-monitor-period secs
      Period for syncing NodeStatus in NodeController. (default 5s)
    --node-startup-grace-period secs
      Amount of time which we allow starting Node to be unresponsive before
      marking it unhealthy. (default 1m0s)
[[}]]

## ServiceAccount [[{security.101,security.aaa]]
- Useful for k8s internal pods (and knative apps?) that want to
  interact with k8s apiserver.
  Running processes (at pod-containers) are authenticated trough
  a given (namespaced) Service Account:
- a 1-hour-expiration-token for API access is provided through
  an "injected" mounted volumeSource added to each pod container
  @ '/var/run/secrets/kubernetes.io/serviceaccount'
  (can be disabled with automountServiceAccountToken)
  note:
     kube-controller-manager --service-account-private-key-file ← set priv. sign.key
     kube-apiserver          --service-account-key-file         ← pub.key

      $ kubectl get serviceAccounts
      NAME      SECRETS    AGE
      default   1          1d   ← service-account controller ensures  it exists in
      ...                         every active namespace.

     $ kubectl apply -f - <<EOF  ← Creating new ServiceAccount:
     apiVersion: v1                (by k8s cluster-namespace-admin)
     kind: ServiceAccount
     metadata:
       name: build-robot
     EOF

- To create additional API tokens:
    kind: "Secret"
    type: "kubernetes.io/service-account-token"
    apiVersion": "v1"
    metadata":
      name: secretForTaskN
      annotations:
        kubernetes.io/service-account.name : "myserviceaccount"
                                              └───────┬──────┘
                                         reference to service account
[[}]]

## Kubeadm [[{cluster_admin.kubeadmin,cluster_admin.IaS,qa.gitops,01_PM.TODO]]
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-reset/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-version/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-alpha/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/

Note: Kubespray offers a "simpler" front-end based on Ansible playbooks to Kubeadm.
Example Tasks:
- Upgrading kubeadm HA clusters from 1.9.x to 1.9.y
@[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha/]
- Upgrading/downgrading kubeadm clusters between v1.8 to v1.9:
@[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-9/]
- Upgrading kubeadm clusters from 1.7 to 1.8
@[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-8/]
- Upgrading kubeadm clusters from v1.10 to v1.11:
@[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-11/]
[[}]]

## Other Admin Tasks [[{cluster_admin,network,01_PM.TODO]]
- TLS Cert Mng:
@[https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/]
@[https://kubernetes.io/docs/tasks/tls/certificate-rotation/]
- kubelet TLS setup:
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/]
- Mng.Cluster DaemonSets:
- Perform a Rollback on a DaemonSet:
@[https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/]
- Perform a Rolling Update on a DaemonSet:
@[https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/]
[[}]]

[[$cluster_admin }]]

## kubespy: observe Kubernetes resources in REAL TIME  [[{TROUBLESHOOTING,APPLICATION.OBSERVABILITY,MONITORING]]

  - What happens when you boot up a Pod? What happens to a Service
    before it is allocated a public IP address? How often is a
    Deployment's status changing?

      kubespy is a small tool that makes it easy to observe how
    Kubernetes resources change in real time, derived from the work we
    did to make Kubernetes deployments predictable in Pulumi's CLI. Run
    kubespy at any point in time, and it will watch and report
    information about a Kubernetes resource continuously until you kill it
[[}]]

## Kubesec: Security risk analysis of resources [[{security.101,security.auditing.kubesec]]
- cli tool
 @[https://github.com/controlplaneio/kubesec]
 @[https://kubesec.io/]

 $ kubesec ./deployment.yml  # multiple YAML documents can be scanned in a single input file.
 [
   {
     "object": "Pod/security-context-demo.default",
     "valid": true,
     "message": "Failed with a score of -30 points",
     "score": -30,
     "scoring": {
       "critical": [
         {
           "selector": "containers[] .securityContext .capabilities .add == SYS_ADMIN",
           "reason": "CAP_SYS_ADMIN is the most privileged capability and should always be avoided"
         }
       ],
       "advise": [
         {
           "selector": "containers[] .securityContext .runAsNonRoot == true",
           "reason": "Force the running image to run as a non-root user to ensure least privilege"
         },
         ...
       ]
     }
   }
 ]

[[}]]

## Gravity (Portable clusters&snahpshots) [[{cluster_admin.backup,security.backup,01_PM.TODO]]
"take a cluster as is and deploy it somewhere else"

- Gravity takes snapshots of Kubernetes clusters, their container
  registries, and their running applications, called "application
  bundles." The bundle, which is just a .tar file, can replicate the
  cluster anywhere Kubernetes runs.

- Gravity also ensures that the target infrastructure can support the
  same behavioral requirements as the source, and that the Kubernetes
  runtime on the target is up to snuff. The enterprise version of
  Gravity adds security features including role-based access controls
  and the ability to synchronize security configurations across
  multiple cluster deployments.

- latest major version, Gravity 7, can deploy a Gravity image into
  an existing Kubernetes cluster, versus spinning up an all-new cluster
  using the image. Gravity 7 can also deploy into clusters that
  aren’t already running a Gravity-defined image. Plus, Gravity now
  supports SELinux and integrates natively with the Teleport SSH
  gateway.
[[}]]


## CRDs (Custom Resource Definition) [[{01_PM.TODO.now]]
https://kubernetes.io/blog/page/17/
KubeDirector: The easy way to run complex stateful applications on Kubernetes
 open source project designed to make it easy to run complex stateful
scale-out application clusters on Kubernetes. KubeDirector is built
using the custom resource definition (CRD) framework and leverages
the native Kubernetes API extensions and design philosophy. This
enables transparent integration with Kubernetes user/resource
management as well as existing clients and tools.
[[}]]




# TODO / UNORDERED NOTES [[{ $undordered_notes ]]

## StatefulSets [[{]]
./StatefullSets.txt
[[}]]

## Kustomize  [[{application.configuration]]
https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/
https://kubectl.docs.kubernetes.io/
  Kustomize lets you customize raw, template-free YAML
  files for multiple purposes, leaving the original YAML untouched and usable as is.

    /Pod patch.yaml
    /Dev patch.yaml    → kustomize → Kubernetes
    /Stagin patch.yaml

### Kustomize vs Helm vs Kubes [[{]]
https://blog.boltops.com/2020/11/05/kustomize-vs-helm-vs-kubes-kubernetes-deploy-tools 
"...Kubes is another tool that handles deployment.
 Kubes has some similar concepts to both Kustomize and Helm and improves on them."

...Kubes introduces a conventional folder structure.
Conventions takes you a long way." 

With Helm, you can also add templating methods with custom helpers.
The helper method definitions are awkward looking, though.
Example:templates/_helpers.tpl
  {{- define "demo.serviceAccountName" -}}
    {{- if .Values.serviceAccount.create }}
    {{- default (include "demo.fullname" .) .Values.serviceAccount.name }}
  {{- else }}
   {{- default "default" .Values.serviceAccount.name }}
  {{- end }}
  {{- end }}

  With Kubes, custom template helper definitions is just Ruby code.
  Example:.kubes/helpers/my_helpers.rbmodule MyHelpers
  def database_endpoint
    case Kubes.env
    when "dev"
      "dev-db.cbuqdmc3nqvb.us-west-2.rds.amazonaws.com"
    when "prod"
      "prod-db.cbuqdmc3nqvb.us-west-2.rds.amazonaws.com"
    end

  Helm supports packaging, Kubes no.
   Kubes supports OCI Image builds, helm doesn't
[[}]]

[[}]]


## GPU and Kubernetes [[{cluster_admin.gpu,01_PM.TODO]]
@[https://www.infoq.com/news/2018/01/gpu-workflows-kubernetes]
@[https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/]
[[}]]

## Skaffold [[{application,01_PM.low_code,qa,01_PM.TODO]]
@[https://www.infoq.com/news/2018/03/skaffold-kubernetes/]

- lightweight client-side tool triggering  highly optimized local deployment
  pipelines when source-code changes are detected by compiling, building
  OCI images, pushing, and deploying the app automatically with policy-based
  image tagging, resource port-forwarding, logging, file syncing, and much more.

  NON-SKAFFOLD                      │  SKAFFOLD
    PIPELINE                        │  PIPELINE
  ============                      │  =========
  · git clone                       │  git clone
  · build app                       │  $ skaffold run   ← Support for: ✓ profiles ✓ env.vars
  · create Dockerfile               │    └─────┬────┘                  ✓ local-user-config ✓ flags
  · build app OCI                   │    Build,deploy, watch for changes. When source changes
  · push image to k8s cluster       │    , roll out updated  objects in local k8s.
  · deploy DDBB from Helm chart     │    Skaffold's inner loop PROVIDES
  · create k8s YAML manifest        │      INSTANT FEEDBACK WHILE DEVELOPING!!!
      for app deployment, services, │
      ...                           │  $ skaffold dev   --port-forward ← like run, forward ports
  · collecting logs and dump into   │  $ skaffold debug --port-forward ← like run, forward ports
  · local console.                  │                                    and add remote debugger
                                                                         (JVM def: localhost:5005)
  ===================
  = Skaffold HOW-TO =
  ===================
  $ cd .../"project_root"
  $ skaffold init --XXenableBuildbacksInit   ← init skaffold using "Cloud Native Buildpacks"
                                               to build OCI image.
                                              './skaffold.yaml' will be created: It can be
                                               customized to use "Cloud Native Buildpacks" Paketo
                                               implementation (e.g. Used by Spring Boot),
                                               define new resources to deploy, ...

apiVersion: skaffold/v2beta13                ← k8s manifest for k8s non-standard object
kind: Config
metadata:
  name: catalog-service
build:                                       ← How to build the App from source
  artifacts:                                   (java+mvn in this example)
    - image: myCompany/myApp01
      buildpacks:
        builder: gcr.io/paketo-buildpacks/builder:base
        env:
          - BP_JVM_VERSION=11.*
deploy:
  helm:
    releases:
      - name: myApp01-ddbb
      remoteChart: bitnami/postgresql
      setValues:
        postgresqlUsername: admin
        postgresqlPassword: admin
        postgresqlDatabase: myApp01-ddbb
        image.tag: 13
  kubectl:
    manifests:
      - k8s/*.yml
[[}]]

## CNCF landscape beginner’s guide [[{01_PM.TODO]]
https://www.cncf.io/blog/2018/11/05/34097/
[[}]]

## IntelliJ k8s plugin [[{]]
@[https://blog.jetbrains.com/idea/2018/03/intellij-idea-2018-1-kubernetes-support/]
[[}]]

## k8s Native IDE [[{]]
https://www.zdnet.com/article/red-hat-introduces-first-kubernetes-native-ide/?ftag=TRE-03-10aaa6b&bhid=28374205867001011904732094012637
[[}]]

## pci/dss compliance [[{security,governance,01_PM.TODO]]
https://www.infoq.com/news/2019/04/kubernetes-pci-dss-compliance
Ana Calin, systems engineer at Paybase, gave an experience report at
QCon London [slides PDF] on how this end-to-end payments service
provider managed to achieve PCI DSS level 1 compliance (the highest)
with 50+ Node.js microservices running on Google Kubernetes Engine
(GKE), and using Terraform for infrastructure provisioning and Helm
for service deployment. Besides pinpointing and addressing some
Kubernetes security shortcomings, another crucial factor was to
challenge the "status quo" of PCI DSS requirements.

PCI DSS is a standard for information security dating back to 2004
that organizations dealing with credit card payments must abide to.
PCI DSS requirements [PDF] do not yet explicitly consider the impact
of Kubernetes or container orchestration in general. Many of them
revolve around the concept of a "server machine" as the base
computation unit and how servers should be secured and
inter-connected. Virtual machines in the cloud could be mapped rather
directly to this "server machine" concept. But when talking about
containers, pods and their transient nature, interpretation of the
same requirements becomes fuzzy.
[[}]]

## k8s cli utilities [[{101,troubleshooting,01_PM.TODO]]
## Configuration patterns [[{]]
Part 2: Patterns for Kubernetes controllers
  https://developers.redhat.com/blog/2021/05/05/kubernetes-configuration-patterns-part-2-patterns-for-kubernetes-controllers
[[}]]

## private Registry  [[{cluster_admin.image_registry,security.secret_mng,network,01_PM.TODO]]
- using a private (image) registry:
@[https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry]
- Pull Image from a Private Registry:
@[https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/]
- imagePullSecrets:
@[https://kubernetes.io/docs/concepts/configuration/secret/]
- method to pass a secret that contains a Docker image registry password
  to the Kubelet so it can pull a private image on behalf of your Pod.
[[}]]





## Kube-router [[{01_PM.TODO,qa]]
https://github.com/cloudnativelabs/kube-router

Since kube-router uses GoBGP, you have access to a modern BGP API
platform as well right out of the box. Kube-router also provides a
way to expose services outside the cluster by advertising ClusterIP
and externalIPs to configured BGP peers.

A key design tenet of Kube-router is to use standard Linux networking
stack and toolset. There is no overlays or SDN pixie dust, but just
plain good old networking. You can use standard Linux networking
tools like iptables, ipvsadm, ipset, iproute, traceroute, tcpdump
etc. to troubleshoot or observe data path. When kube-router is ran as
a daemonset, image also ships with these tools automatically
configured for your cluster.
[[}]]

## K8s Sec. Policies [[{101,security.governance,01_PM.TODO]]
- Security Policies - To define & control security aspects of Pods,
  use Pod Security Policy (available on v1.15). According Kubernetes
  Documentation, it would enable fine-grained authorization of pod
  creation and updates. Defines a set of conditions that a pod must run
  with in order to be accepted into the system, as well as defaults for
  the related fields. They allow an administrator to control the
  following:
        Running of privileged containers
        Usage of host namespaces
        Usage of host networking and ports
        Usage of volume types
        Usage of the host filesystem
        Restricting escalation to root privileges
        The user and group IDs of the container
        AppArmor or seccomp or sysctl profile used by containers
[[}]]

## Falco Security [[{security.101,security.governance.policies,02_doc_has.comparative,01_PM.TODO]]
@[https://www.infoq.com/news/2020/01/falco-security-cncf/]
- Sysdig Falco provides intrusion and abnormality
  (app|container|VM, ...) detection for Kubernetes, Mesosphere,
  Cloud Foundry, ...  protecting from malicious activity.
- 2018: CNCF Sandbox , 2020-01: CNCF Incubation-level.
- It leverages open source Linux kernel instrumentation to monitor
  the stream of system calls from the kernel.
  Running in user-space, it is able to augment the kernel
  data with other input streams such as container runtime metrics and
  Kubernetes metrics alerting on any behaviour that makes Linux system
  calls.
- alerting rules make use of Sysdig's filtering expressions
  to identify potentially suspicious activity.
  (process starting a shell inside a container, container
   running in privileged mode, or an unexpected read of a sensitive
   file).
- Falco can notify via Slack, Fluentd, and NATS.

- Falco rule:
  · condition field: filter applied to each system call.
    e.g.: attempts to start a shell process within a container:
    - rule: shell_in_container
      desc: notice shell activity within a container
      condition: container.id != host and proc.name = bash
      output: shell in a container (user=%user.name
    container_id=%container.id container_name=%container.name
    shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline)
      priority: WARNING

- Falco 0.4.0 communication with Kubernetes and Mesos servers is
  possible allowing for creation of policies using the context of those
  frameworks. This allows specifying properties such as container ids,
  image names, Kubernetes namespaces, services, deployments, or Mesos
  frameworks.

- While similar to other tools that allow for declaring security   [comparative]
  policies such as SELinux, AppArmor, or auditd, Falco has some
  differences. As Mark Stemm, software engineer at Sysdig, notes:
  - Tools like seccomp, seccomp-bpf, SELinux, and AppArmor fall into the
    enforcement category in that they will alter the behaviour of
    processes if they are found to violate the defined rules. Falco and
    other tools, such as auditd, fall in the auditing category as they
    will notify when detecting a violation.
  - Falco Falco runs in user space, using a kernel module to
    obtain system calls, while other tools perform system call
    filtering/monitoring at the kernel level." This allows Falco to have
    more available data to be used within its policies as noted
    previously.
[[}]]

## BrewOPA: Admin Policy made easy [[{security.governance,01_PM.low_code,01_PM.TODO]]
@[https://www.zdnet.com/article/kubernetes-administration-policy-made-easy-with-brewopa/]

- Kubernetes administration policy made easy with brewOPA

- Administering policies across Kubernetes and other cloud native environments
  isn't easy. Now, Cyral wants to take the trouble out of the job with brewOPA.
[[}]]

## limits/request by example [[{101,01_PM.TODO]]
Understanding Kubernetes limits and requests by example
https://sysdig.com/blog/kubernetes-limits-requests/
[[}]]

## k8s new features [[{01_PM.TODO]]
https://lwn.net/Articles/806896/
[[}]]

## git-sync "sidecar" [[{qa.gitops,application.sidecar,01_PM.TODO]]
@[https://github.com/kubernetes/git-sync]
- simple command that pulls a git HEAD|tag|hash repo into
  a local dir. It is a perfect "sidecar" container in Kubernetes -
- it can periodically pull files down from a repository so that an
  application can consume them.
  It will only re-pull if the target of the run has changed in
  the upstream repository. When it re-pulls, it updates the destination
  directory atomically. In order to do this, it uses a git worktree in
  a subdirectory of the --root and flips a symlink.
- A webhook call upon successful git repo synch is allowed after the
  symlink is updated.
[[}]]

## Volume Snapshot,Restore 1.12+ [[{storage.101,security.backups,01_PM.TODO]]
  @[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support]
[[}]]

## kubervisor [[{qa,01_PM.TODO]]
https://github.com/AmadeusITGroup/kubervisor
AmadeusITGroup/kubervisor: The Kubervisor allow you to control which
pods should receive traffic or not based on anomaly detection.
It is a new kind of health check system.
[[}]]

## kubectl recipes [[{101.kubectl,01_PM.TODO]]
  $ kubectl get events --all-namespaces  [[{monitoring.cluster,monitoring.application]]
NAMESPACE     LAST SEEN   TYPE      REASON                    OBJECT                              MESSAGE
default       29d         Normal    NodeNotReady              node/node1                          Node node1 status is now: NodeNotReady
default       7d4h        Normal    Starting                  node/node1                          Starting kubelet.
default       7d4h        Normal    NodeHasSufficientMemory   node/node1                          Node node1 status is now: NodeHasSufficientMemory
default       7d4h        Normal    NodeHasNoDiskPressure     node/node1                          Node node1 status is now: NodeHasNoDiskPressure
...
kube-system   7d4h        Normal    Pulled                    pod/calico-kube-controllers-...     Container image ".../kube-controllers:v3.15.1" already present ...
kube-system   7d4h        Normal    Created                   pod/calico-kube-controllers-...     Created container calico-kube-controllers
kube-system   7d4h        Normal    Started                   pod/calico-kube-controllers-...     Started container calico-kube-controllers
...
kube-system   7d4h        Warning   FailedCreatePodSandBox    pod/dns-autoscaler-66498f5c5f-dn458 Failed to create pod sandbox: rpc error: code ...
...
kube-system   7d4h        Normal    LeaderElection            endpoints/kube-controller-manager   node1_e4a58997-c39f-430d-a942-31d53124c5d5 became leader
kube-system   7d4h        Normal    LeaderElection            lease/kube-controller-manager       node1_e4a58997-c39f-430d-a942-31d53124c5d5 became leader
...
kube-system   34m         Warning   FailedMount               pod/kubernetes-dashboard-...        MountVolume.SetUp failed for volume "kubernetes-dashboard-certs"...
                                        [[monitoring.cluster}]]

$ kubectl get endpoints --all-namespaces [[{monitoring.network]]
  NAMESPACE     NAME                        ENDPOINTS                                           AGE
  default       kubernetes                  192.168.1.2:6443                                    202d
  kube-system   coredns                     10.233.90.23:53,10.233.90.23:9153,10.233.90.23:53   202d
  kube-system   dashboard-metrics-scraper   10.233.90.24:8000                                   202d
  kube-system   kube-controller-manager     <none>                                              202d
  kube-system   kube-scheduler              <none>                                              202d
  kube-system   kubernetes-dashboard        10.233.90.22:8443                                   202d
                                         [[monitoring.network}]]

$ kubectl get deployments --all-namespaces
  NAMESPACE     NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
  kube-system   calico-kube-controllers      1/1     1            1           202d
  kube-system   coredns                      1/2     2            1           202d
  kube-system   dns-autoscaler               1/1     1            1           202d
  kube-system   kubernetes-dashboard         1/1     1            1           202d
  kube-system   kubernetes-metrics-scraper   1/1     1            1           202d
[[}]]


## OCI "vs" docker "vs" k8s [[{02_doc_has.diagram]]
 https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/
  ┌──────┐                   ┌──────────┐
  │docker│                   │kubernetes│
  └──┬───┘                   └────┬─────┘
     │                            │
     │                            │
     │                       ┌────▼─────┐   CRI: K8s API
     │                       │ Container│
     │                       │   Runtime│
     │                       │ Interface│
     │                       └─┬──┬─────┘
     │                         │  │
     │        ┌────────────────┘  │
     │        │                   │
     │    ┌───▼──┐                │
     │    │  CRI │                │         CRI "Implementation"
     │    │plugin│                │         - containerd: From docker Comp.
   ┌─▼────┴─────┬┘             ┌──▼──┐        manages storage and network,
   │ containerd │              │CRI-O│        supervises running containers
   └──────┬─────┘              └──┬──┘      - CRI-O: From IBM/RedHat/ ...
          │                       │
          │                       │
          │                       │
┌─────────▼───────────────────────▼───┐     Standard spec for container
│ Open Container Initiative (OCI) spec│     *Images* and running container
└───────────┬─────────────────────────┘
            │
            │
         ┌──▼───┐                           OCI reference tool for spawing
         │ runc │                           and running containers.
         └──┬───┘
            │
       ┌────┴──────┐
       │           │
       │           │
   ┌───▼─────┐  ┌──▼──────┐
   │container│  │container│                 Containers
   └─────────┘  └─────────┘
                                                                       [[02_doc_has.diagram}]]

## Kubestriker: security auditing  [[{]]
  https://www.helpnetsecurity.com/2021/05/04/security-kubernetes/ 
[[}]]



## audits log events 1.11+ [[{security.AUDITING]]
[[}]]

## Cluster Debug/Troubleshoot [[{cluster_admin.troubleshooting,01_PM.TODO.clean]]
See also:
- @[https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/]
- @[https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/]
[[}]]


@[https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/]
  - A VM node can be ‘tainted’ (marked) with a certain value so
    that pods with a related marker are not scheduled onto it. Unused
    nodes would be brought down faster by the Kubernetes standard cluster
    autoscaler when they are marked. The scale-up configuration parameter
    is a threshold expressed as a percentage of utilization, usually less
    than 100 so that there is a buffer.  Escalator autoscales the compute
    VMs when utilization reaches the threshold, thus making room for
    containers that might come up later, and allowing them to boot up
    fast.

## Resource Quotas
* resource quota feature can be configured to limit the total amount of resources
  that can be consumed. In conjunction namespaces, it can prevent one team from
  hogging all the resources.

## Kubernetes Security Posture Management [[{]]
  https://sysdig.com/learn-cloud-native/kubernetes-security/kspm/
  Kubernetes security posture management, or KSPM, is the use of
  security automation tools to discover and fix security and compliance
  issues within any component of Kubernetes.

  For example, KSPM could detect misconfigurations in a Kubernetes RBAC
  Role definition that grants a non-admin user permissions that he or
  she should not have, like the ability to create new pods. Or, a KSPM
  tool could alert you to an insecure Kubernetes network configuration
  that allows communication between pods in different namespaces, which
[[}]]
  would typically not be a setting you would want to enable.

## ValidKube: Enforce YAML Best Practices [[{qa,]]
https://www.infoq.com/news/2022/02/validkube-kubernetes-yaml/ 
- browser-based tool.
[[}]]

## kubernetes patterns book [[{qa]]
@[https://github.com/k8spatterns/examples]
[[}]]

# All components of a typical k8s app!!
@[https://medium.com/geekculture/how-to-deploy-spring-boot-and-mongodb-to-kubernetes-minikube-71c92c273d5e]
[[}]]

[[$undordered_notes }]]
