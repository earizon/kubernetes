## Summary:
* Istio solves similar problems to existing "SERVICE MESH" libraries
  like Netflix OSS or Spring Cloud ( Service Discovery, Circuit Breaker,
  Intelligent Routing, Client Side Load Balancing, distributed tracing,
  monitoring, ...) by replacing "embedded" libraries with (Envoy) sidecards.
* MAIN ADVANTAGE: Provides for low-code polyglot architectures.

* Istio architecture:
  * CONTROL PLANE: Manage service mesh configuration.
  * DATA    PLANE: (Envoy sidecar proxies) manage dataflow
    (translates, forwards and monitor every network packet
     flowing among application services)

* Out of the box k8s offers Service Discovery, Service Invocation and
  Service Elasticity. On top of them Istio offer:
  * Observability + (Alert)Notification: Prometheus + Grafana
  * Distributed tracing: Jaeger + Zipkin
    KEY-POING: **Istio intercepts all requests/responses and sends them
                 to Jaeger**

## Installing Istio [[{devops.101]]
1. Download and install inside k8s cluster:
```
  $ URL=https://istio.io/downloadIstio
  $ curl -L ${URL} | sh -         # download istio (contains binaries + samples)
  $ cd istio-1.17.2/              #
  $ export PATH=$PWD/bin:$PATH    # add istio to path
  $ istioctl install -y \         # install "inside" k8s cluster.
    --set profile=demo \          <·· demo is a pre-packaged istio profile
  ...
  ✔ Istio core installed          NOTE: To remove/clean Istio install:
  ✔ Istiod installed              $ istioctl uninstall -y --purge
  ✔ Egress gateways installed     $ kubectl delete namespace istio-system
  ✔ Ingress gateways installed
  ✔ Addons installed
  ✔ Installation complete
```
2. Wait until all Pods in **istio-system namespace** are in RUNNING
```
  $ kubectl  get pods -n istio-system
  NAME
  istiod-76bf8475c-xph     <·· MAIN CONTROLLER (MAIN Loop)
                               also known as Istio Pilot
                               maps "desired" configurations to
                               (side-car) proxy-specific configs.
                               Exposes also the Envoy's native "xDS API"
                               - It also handles MTLS Certs following
                                 https://spiffe.io specs.
                                 (Sec.Prod.Iden.Framework 4 Everyone)
  istio-egressgateway-b... <·· Outgoing  Traffic from Mesh (Cluster wide Envoy proxy)
  istio-ingressgatewayf... <·· incomming Traffic to   Mesh (Cluster wide Envoy proxy)
                               It works like:
                               ┌ curl
                               ·
                               └··> K8s  ··> Istio ··> Istio ··> Istio ┐   k8s Service
                                    Load     ingress   Virtual   Dest. ├·> Pod1
                                    Balancer gateway   Service   Rule  └·> Pod2

  istio-tracing-9dd6c44... <·· Distributed tracing system to visualize (application layer)
                               request flow/s through the mesh. Access like:
                               $ istioctl dashboard jaeger
                               (kubectl port-forward -n istio-system ... will work too)
  kiali-d45468dc4-fl8j     <·· global overview (how services are connected,
                               how they perform, Istio resources registered, ...)
                               $ kubectl port-forward -n istio-system kiali... 20001:20001

  prometheus-74d44d84dd    <·· Collects&stores generated metrics as Time-Series data
  grafana-b54bb57b9-k5     <·· Visualizes metrics generated by the proxies
                               and collected by Prometheus.  Access it like:
                               $ kubectl port-forward \
                                 -n istio-system \
                                 grafana-b54bb57b9-k5qbm 3000:3000
```
3. Inject Istio sidecar automatically (**NO need to change Deployments!!!**)
   into pods for a given NS ("default" in example).
   Adding a namespace label instructs Istio to automatically inject
   Envoy sidecar proxies when deploying the application later:
```sh
 $ kubectl label namespace default istio-injection=enabled
```
[[devops.101}]]

# Istio + k8s "Application" how to  [[{101]]
1. Deploy an standard K8S deployment v1/v2/... + Service:
   ```
   $ kubectl apply -f service.yml       -n default
                      (LoadBalancer service)
   $ kubectl apply -f deployment-v1.yml -n default
   $ kubectl apply -f deployment-v2.yml -n default
   ```
2. Check deployment:
   ```
   $ kubectl get pods -n default
   NAME             READY   STATUS    ...
   ..
                    ^^^
                    1 service container +
                    1 istio sidecar container (if autoinject for namespace has been enabled)
   ```
3. Create `DestinationRule` subsets:
   ```
   $ editor destination-rule-v1-v2.yml
 + apiVersion: networking.istio.io/v1alpha3
 + kind: DestinationRule
 + metadata:
 +  name: serviceA
 + spec:
 +   host: serviceA                  <·· DNS name specified in k8s Service.
 +   subsets:
 +   - labels:
 +       app.kubernetes.io/version: v1.0.0
 +     name: version-v1               <·· "virtual" name for subset
 +   - labels:
 +       app.kubernetes.io/version: v2.0.0
 +     name: version-v2
 +   trafficPolicy:                   <·· "optional"
 +     loadBalancer:
 +       simple: ROUND_ROBIN          <·· ROUND_ROBIN*, RANDOM, WEIGHTED, LEAST_REQUESTS
 +     connectionPool:                <·· Reuse TLS handshakes, tcp connections,...
 +       tcp:                             (pool is monitored by Envoy)
 +         maxConnections: 100
 +         connectTimeout: 30ms
 +         tcpKeepalive:
 +           time: 7200s
 +           interval: 75s
 +       http:                             unused HTTP connections,...
 +         http1MaxPendingRequests: 3
 +         maxRequestsPerConnection: 3
 +       tcp:
 +         maxConnections: 3           <·· Limiting concurrent connections
 +     outlierDetection:               <·· Configure CIRCUIT BREAKER to remove failing Pod
 +       #                                 for balancer.
 +       interval: 1s                      <· Open/break it if an error
 +       consecutive5xxErrors: 1              occurs within a 1sec window
 +       baseEjectionTime: 3m
 +       maxEjectionPercent: 100           <· tripping the service for three minutes.
 +                                            After, circuit will be half-opened
 +                                            If it fails again, the circuit remains open
 +                                            otherwise it is reestablished.





   $ kubectl apply -f destination-rule-v1-v2.yml -n default
   ```
4. Test App:
   ```
  $ curl LoadBalan:1234/X/${id} # (v1 response)
  $ curl LoadBalan:1234/X/${id} # (v2 response)
  $ curl LoadBalan:1234/X/${id} # (v1 response)
  $ curl LoadBalan:1234/X/${id} # (v2 response)
  ...
   ```
5. Create a "virtual service":
   ```
   $ editor virtual-service-v1.yml # Forwarding
 + apiVersion: networking.istio.io/v1alpha3
 + kind: VirtualService              <·· Custom Resource installed with Istio install
 + metadata:
 +  name: serviceA
 + spec:
 +  hosts:
 +  - serviceA
 +  http:
 +  - route:               <··· Forward-rule (match inside rule)
 +    - destination:       <··· where to route to
 +        host: serviceA
 +        subset: version-v1   <· SEND ALL TRAFFIC TO serviceA v1
 +      weight: 100            <· ┘
 + #  - destination:         <·· uncomment and change weight
 + #      host: serviceA         to, for example 75/25 for
 + #      subset: version-v2     canary releases.
 + #    weight: 25               # test setup:
 + #                             $ curl LoadBalan:1234/X/${id} # (v1 response)
 + #                             $ curl LoadBalan:1234/X/${id} # (v1 response)
 + #                             $ curl LoadBalan:1234/X/${id} # (v1 response)
 +  - route:               <·· Just another (HTTP) forward-rule
 +    ...
 +  - match:               <·· Rewrite rule matching path-part. (rule inside match)
 +    #                        AND match: nest N conditions under single match attribute
 +    #                        OR  match: parallel match
 +    - uri:
 +        prefix: /hello         <·· Only prefix is rewritten in this case.
 +    rewrite:
 +      uri: /
 +    route:
 +    - destination:
 +        host: webservice
 +        subset: v1
 +      timeout: 1s        <·· Optional when we know for sure that the remote
 +                             service is failing after such timeout.
 +      retries:           <·· Retry policy.  Availability can be increased (sometimes)
 +        #                    by simply retrying the failed request one more time.
 +        #                    there are obvious scenarios where retries should be avoided
 +        #                    (eg: non idempotent services, expensive computations, ..)
 +        attempts: 2          <·· disable after 2 attempts
 +        perTryTimeout: 0.5s
 +        retryOn: 5xx
 +  - match:               <·· request-Rewrite rule matching header
 +    - headers:               * Other match includequery params, HTTP method, scheme, ...
 +      x-canary-launch:
 +       exact: "v3"
 +      x-target:          <·· AND condition (inside headers)
 +       exact: "prod"
 +    - queryParams:       <·· OR condition (parallel to header)
 +        ver:
 +          exact: v1
 +       method:
 +         exact: GET
 +    route:
 +    - destination:
 +        host: website
 +        subset: v3

   $ kubectl apply -f virtual-service-v1.yml -n default

   $ $istioctl get virtualservices  # check
```
[[101}]]

# Authentication [[{ $authentication ]]
## Authentication: zero-code HTTPs mutual Authentication [[{]]

* Istio in charge of managing certificates, CAs and revoking/renewing certificates.

1. STEP 1) validate that mTLS is enabled:
```
$ istioctl experimental authz check book-service-5cc59cdcfd-5qhb2 -a
LISTENER[FilterChain]   HTTP ROUTE                        ALPN                                mTLS (MODE)         AuthZ (RULES)
...
virtualInbound[5]       inbound|8080|http|book-service... istio-http/1.1...  noneSDS: default yes  (PERMISSIVE)   no    (none)
...                                                                                           \---------------/
                                                                                              mTLS configured with
                                                                                              permissive strategy
```
[[}]]

## Authenticating end-users with JWT [[{]]
1. Create RequestAuthentication policy resource.
```
$ editor request-authentication-jwt.yml
apiVersion: "security.istio.io/v1beta1"
kind: "RequestAuthentication"
metadata:
 name: "bookjwt"
 namespace: default
spec:
 selector:
   matchLabels:                                    policy ensures that if the "Authorization" header
     app.kubernetes.io/name: book-service          contains a JWT token, it must be valid, not expired,
 jwtRules:                                     <·· issued by the correct issuer, and not manipulated.
 - issuer: "testing@secure.istio.io"               <··· Valid issuer of the token.
   jwksUri: "https://.../raw/jwks.json"            <··· URL where public keys are registered
                                                        for validation

$ kubectl apply -f request-authentication-jwt.yml -n default
```
1. Test with a non-valid JWT:
```
$ curl .../book/1 \
   -H "Authorization: Bearer ${WRONG_JWT}" ... <·· invalid token 401 Unauthorized response
$ curl .../book/1 \
   -H "Authorization: Bearer ${VALID_JWT}" .. <··   valid token
  (response)
```
[[}]]

## Authorizing end-users with RBAC model. [[{]]
 Let’s create . Create a file with name authorization-policy-jwt.yml:
1. Create AuthorizationPolicy allowing only valid request with claim role set to customer in JWT
```
$ editor authorization-policy-jwt.yml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: require-jwt
 namespace: default
spec:
 selector:
   matchLabels:
     app.kubernetes.io/name: book-service
 action: ALLOW
 rules:
 - from:
   - source:
      requestPrincipals: ["testing@secure.istio.io/testing@secure.istio.io"]
   when:
   - key: request.auth.claims[role]
     values: ["customer"]

$ kubectl apply -f authorization-policy-jwt.yml
```
[[}]]

[[ $authentication }]]




---------------------------------------------------------------
$ istioctl kube-inject -f myAppResources.yaml  # *yaml contains Deployment/s,Serice/s, ...
           ^^^^^^^^^^^
           enrich Pods with istio (Envoy) sidecar proxy + additional components:

# Envoy Filters:
 Filters enable Envoy to perform various operations such as routing, translating protocols, generating statistics, etc., on a received message. Each port listener configures its own set of filters. All these filters are combined for a filter chain, which is invoked for every TCP message. Envoy has a large set of out-of-the-box filters. These filters can be broadly classified as follows:

## Listener filters (L4): invoked as part of a handshake in a connection request. Responsible for TLS inspection, remote destination ,...
## Network filters: invoked for every TCP message after a connection. Responsible for application authorization, rate limiting, TLS authentication,.
## Filters for application-specific protocols like MySQL and MongoDB exists in order to gather statistics, perform role-based access, etc.
## HTTP filters (L7): gzip compression, gRPC to JSON translation, etc.

# Istio Mixer [[{istio.mixer]]
* platform-independent component to collect telemetry,
* It currently supports the following three use cases:
  * Precondition checking
  * Quota management, such as API limits
  * Telemetry reporting such as logs and requests

* It also enforces the authorization policy.

## Mixer Adapters  [[{]]
* Abstract Istio Mixer from current providers.
  (Zipkin, StatsD, Stackdriver, CloudWatch, ...)
  A complete list of adaptars can be found at:
@[https://istio.io/docs/reference/config/policy-and-telemetry/adapters/]

  ```
  $ kubectl get crd -listio=mixer-adapter  # list available mixer adapters
  NAME                              CREATED AT
  adapters.config.istio.io          2019-07-14T07:46:10Z
  bypasses.config.istio.io          2019-07-14T07:45:59Z
  circonuses.config.istio.io        2019-07-14T07:45:59Z
  deniers.config.istio.io           2019-07-14T07:46:00Z
  fluentds.config.istio.io          2019-07-14T07:46:00Z
  Kubernetes envs.config.istio.io   2019-07-14T07:46:00Z
  listcheckers.config.istio.io      2019-07-14T07:46:00Z
  memquotas.config.istio.io         2019-07-14T07:46:01Z
  noops.config.istio.io             2019-07-14T07:46:01Z
  opas.config.istio.io              2019-07-14T07:46:02Z
  prometheuses.config.istio.io      2019-07-14T07:46:02Z
  rbacs.config.istio.io             2019-07-14T07:46:03Z
  redisquotas.config.istio.io       2019-07-14T07:46:03Z
  servicecontrols.config.istio.io   2019-07-14T07:46:04Z
  signalfxs.config.istio.io         2019-07-14T07:46:04Z
  solarwindses.config.istio.io      2019-07-14T07:46:04Z
  stackdrivers.config.istio.io      2019-07-14T07:46:05Z
  statsds.config.istio.io           2019-07-14T07:46:05Z
  stdios.config.istio.io            2019-07-14T07:46:05Z


  ```

### Logging Backends
* Amazon CloudWatch: Sends logs to CloudWatchLogs.
* Fluentd daemon:
* Prometheus??

### Quota Backends
(keep track of different endpoint quotas)
* Redis Quota: rate-limiting quota for a fixed or rolling
  window algorithm.

### Authorization Backend
* List: Simple deny/allow list for IP addresses or regex patterns.

### (Pod,...) Telemetry backend
* StatsD: delivers metric data to a StatsD monitoring back end.

### (Data plane) Attributes
* Attribute: smallest data chunk that defines a property of a request.
* They include: request path, destination IP address, response code,
  response size, request size, ...
  (Used by Mixer & Envoy)

### Configuration Model
* Based on adapters + templates.
* Templates: define how attributes are fed into the adapters.
  * Handlers: define the configuration of an adapter.
    ("how the adapter needs to be invoked")
    e.g: In StatsD, requests count can be one attribute to be fed
      The list of available handlers depends on the adapters deployed
    in the service mesh. Also, we need to refer to the adapter
    documentation to know what configuration options are available. As an
    example let's look at the fluentds.config.istio.io adapter. The
    adapter is used to send access logs to the fluentd aggregator demon.
[[}]]


  * Instances:
    * raw data "understood" by an adapter is compiled in four templates,
      each one with a set of properties that can be captured by the adapter.
    * The template also assigns default values for missing attributes.
    * Each adapter has a list of templates that can be used to send the data.
      Thus, an instance can be defined as a mapping of instance attributes
      to template inputs associated to the adapter.
    ```
    $ kubectl get crd -listio=mixer-instance # sow available templates
    NAME                                    CREATED AT
    apikeys.config.istio.io                 2019-07-14T07:46:06Z
    authorizations.config.istio.io          2019-07-14T07:46:06Z
    checknothings.config.istio.io           2019-07-14T07:46:06Z
    edges.config.istio.io                   2019-07-14T07:46:07Z
    instances.config.istio.io               2019-07-14T07:46:11Z
    Kubernetes es.config.istio.io           2019-07-14T07:46:06Z
    listentries.config.istio.io             2019-07-14T07:46:07Z
 ┌> logentries.config.istio.io              2019-07-14T07:46:07Z
 ·  metrics.config.istio.io                 2019-07-14T07:46:08Z
 ·  quotas.config.istio.io                  2019-07-14T07:46:08Z
 ·  reportnothings.config.istio.io          2019-07-14T07:46:08Z
 ·  servicecontrolreports.config.istio.io   2019-07-14T07:46:08Z
 ·  tracespans.config.istio.io              2019-07-14T07:46:09Z
 ·
 └· $ kubectl get logentries.config.istio.io --all-namespaces
    NAMESPACE      NAME           AGE
    istio-system   accesslog      41d
    istio-system   tcpaccesslog   41d

    # look at accesslog-definition to fetch which details are captured.
    $ kubectl get logentries.config.istio.io accesslog -n istio-
    system -o yaml
    apiVersion: config.istio.io/v1alpha2
    kind: logentry
    metadata:
      // REMOVED FOR BREVITY
    spec:
      monitored_resource_type: '"global"'
      severity: '"Info"'
      timestamp: request.time
      variables:
        apiClaims: request.auth.raw_claims | ""
        apiKey: request.api_key | request.headers["x-api-key"] | ""
        .......
        destinationApp: destination.labels["app"] | ""
        destinationIp: destination.ip | ip("0.0.0.0")
        destinationName: destination.name | ""
        latency: response.duration | "0ms"
        method: request.method | ""
        protocol: request.scheme | context.protocol | "http"
        receivedBytes: request.total_size | 0
        // REMOVED for brevity

    ```

  * Rules: eg: "we want to push a request count for only one service" maps to rule:
    `destination.service.name == <Service Name>`
[[}]]
[[istio.mixer}]]

## Pilot [[{]]
* it does the routing, provides service discovery, and facilitates
  timeouts, retries, circuit breakers, and so on.
* Pilot separates out the platform-specific way of service discovery
  from Istio, THUS ALLOWING ISTIO TO RUN ON MULTIPLE ENVIRONMENTS SUCH
  AS KUBERNETES, NOMAD, AND SO ON

Comparative from
https://stackoverflow.com/questions/48639660/what-is-the-difference-between-mixer-and-pilot-in-istio

* Pilot gets in charge of:
  * Routing (eg:90% of traffic goes to v1, 10% goes to v2)
    also also:
    * a) rewrite
    * b) redirect
  * Support for microservices (dev/deploy/test)
    * a) timeouts
    * b) retries
    * c) circuit breakers
    * d) load balancing
    * e) fault injection (for testing)
* Mixer is responsible for:
  * Logging, Distributed Tracing, Telemetry
  * Policy enforcement
* Citatel:
  * Secure communication and strong identity.

          │Citadel│··PKI·······┐
                               v
|Pilot|·> (routing,config) ··>|Envoy|··(req/res trace)·>|Mixer|
                                 ^│                       ^│
                                 │└ Q:allow/deny request?─┘│
                                 └─ A:YES/NO ──────────────┘

[[}]]

## Citadel [[{]]
* provides features to encrypt requests within the Istio mesh.
  "injecting" or "pushing" certificates to Envoy
* provides role-based access control for services within the mesh.
[[}]]

## Galley [[{]]
* management plane abstracting the Istio mesh of a configuration's input
  from the user as well as from the underlining environment.
[[}]]


## Istio cli Summary [[{]]
$ istioctl get virtualservices

$ istioctl authn tls-check $pod_name
           └───┴─·· interact with the Istio authentication policies.

$ istioctl deregister <service-name> <ip-to-be-removed>
$ istioctl   register <service-name> <ip-to-be-added> <port>

$ istioctl experimental auth check <pod-name>

$ istioctl experimental convert-ingress # k8s ingress ···> Istio VirtualService
                                          config. (best effort conver)

$ istioctl experimental dashboard grafana

$ istioctl experimental metrics $serv_name # depends on Prometheus.
$ istioctl kube-inject ... # converts k8s config ··> Istio config
                           # by injecting the Envoy sidecar.

$ istioctl proxy-config bootstrap|cluster|endpoint|listener|route
$ istioctl validate ...
[[}]]


## Ingress Rule [[{]]
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: webapp-gateway
spec:
  hosts:
  - "*.greetings.com"    <··· allow external HTTP traffic for *.greetings.com
  selector:                   (from k8s load balancer to istio Gateway)
  istio: ingressgateway
  servers:
  - port:
  number: 80
  name: http
  protocol: HTTP

apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
 name: webservice-wtdist-vs
spec:
 hosts:
 - webservice
 - webservice.greetings.com
 gateways :             <··· "attach" ingress gateway to Virt.Service
 - webapp-gateway
 http:
[[}]]

## Service Entry (Ext.Services) [[{]]
* Default Istio config allows all traffic to external services        [[{security]]
  bypassing the egress proxy for all services unknown to the mesh.
  Requests are directly handled by the application pod network.
  To switch to deny-by-default (502 Bad Gateway) execute:
  $ SED="s/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g"
  $ kubectl get configmap \
    istio -n istio-system -o yaml | \
    sed ${SED} | \
    kubectl replace -n istio-system -f -

  $ kubectl get configmap istio \     <·· check changes has been applied
    -n istio-system -o yaml
  mode: REGISTRY_ONLY ..                                               [[}]]

* To explicetely allow external service:
  ```
  ---
  apiVersion: networking.istio.io/v1alpha3
  kind: ServiceEntry
  metadata: { name: wikipedia }
  spec:
   hosts: [ "en.wikipedia.org" ]
   ports: { number: 443 }
   name: https
   protocol: HTTPS
   resolution: DNS           <··· DNS or static (IP address)
   location: MESH_EXTERNAL   <·· MESH_EXTERNAL(Istio disables mTLS) | MESH_INTERNAL
  ```
[[}]]

## Egress Gateway  [[{]]
* ServiceEntry restrict global access to a limited set of external services.
* Egress: Every request must be inspected to restrict access to only authorized ones.

---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway                             <·· handle requests matching a given host& port
metadata:
 name: wikipedia-egressgateway
spec:
  selector:
  istio: egressgateway
  servers: [ { port: } ]
  number: 80
  name: http
  protocol: HTTP
  hosts: [ "en.wikipedia.org" ]
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService                    <··· "glue between Gateway and ServiceEntry.
                                             1) VirtualService handles all requests for en.wikipedia.org.
                                                (it applies to the gateway and all sidecar proxies).
                                             2) Requests from sidecars are routed to istio-egressgateway
                                             Pod
                                              ··> Envoy Side(VirtualService)
                                                ··> istio-egressgateway (with ServiceEntry)
                                                  ··> wikipedia
metadata:
 name: wiki-egress-gateway
spec:
 hosts: [ "en.wikipedia.org" ]
 gateways:
 - wikipedia-egressgateway
 - mesh
 http:
 - match:
 - gateways:
 - mesh
 port: 80
 route:
 - destination:
 host: istio-egressgateway.istio-system.svc.cluster.local
 port:
 number: 80
 weight: 100
 - match:
 - gateways:
 - wikipedia-egressgateway
 port: 80
 route:
 - destination:
 host: en.wikipedia.org
 port:
 number: 80
 weight: 100
---
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry          <··· allow en.wikipedia.org to be accessed by services inside the mesh
metadata:
 name: wikipedia
spec:
 hosts:
 - en.wikipedia.org
 ports:
 - number: 80
 name: http
 protocol: HTTP
 resolution: DNS
 location: MESH_EXTERNAL
[[}]]

## fault injection (Chaos testing) [[{]]
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: webservice
spec:
  hosts:
  - webservice
  http:
  - fault:
      abort:
        httpStatus: 503
        percent: 50           <·· Simulate failures  for 50% of request. No code change needed.
  - fault:
      delay:
        fexedDelay: 5s        <·· Simulate 5s delays for 50% of request. No code change needed.
        percent: 50

    route :
    - destination:
        host: webservice
    retries:
      attempts: 0
[[}]]

## Prometheus [[{]]
* OOSS monitoring and alerting (through Grafana) tool.
* Time Series engine with data grouped on the basis of metric name and labels (key-value pairs).
* query language to analyze the data
* An alerting system to send notifications based on analysis done in the second step

Prometheus INPUT (collects metrics):
* Pull model over HTTP.

* preconfigured to scrape Mixer endpoints to collect exposed metrics.
  - istio-telemetry.istio-system:42422/metrics: This returns all Mixer-generated metrics.
  - istio-telemetry.istio-system:15014/metrics: This returns all Mixer-specific metrics.
  - istio-proxy:15090/metrics: raw stats generated by Envoy.
  - istio-pilot.istio-system:15014/metrics:   Pilot-generated metrics
  - istio-galley.istio-system:15014/metrics   Galley-generated metrics
  - istio-policy.istio-system:15014/metrics:  all policy-related metrics
  - istio-citadel.istio-system:15014/metrics: all Citadel-generated metrics

WARN:  For a production environment, it may be better to set up Prometheus separately from istio.

* the predefined configuration has instances and handlers connected by
  rules. The predefined instances provided are:
  INSTANCE              : Captures ...
  ──────────────────────┼────────────────────────────────────────────────────
  - accesslog           : log entry for request src&dest details
  - attributes          : source and destination pod, workload, and namespace details
  - requestcount        : number of source to destination requests
  - requestduration     : response time of ALL THE CALLS in the mesh
  - requestsize         :
  - responsesize        :
  - tcpaccesslog        : metrics of the TCP request
  - tcpbytereceived     : bytes received by the destination in the TCP request
  - tcpbytesent         : bytes sent by the source in the TCP request
  - tcpconnectionsclosed: the number of times the TCP connection is closed
  - tcpconnectionsopened: the number of times TCP connection is opened

## Prometheus Custom Metrics [[{]]
* there are scenarios when the mixed metrics to Prometheus need to be recalculated to make sense.

### Eg scenario: double all the requests to the webapp service.

apiVersion: config.istio.io/v1alpha2
kind: instance                              <··· Mixer instance
metadata:
  name: requestdouble
  namespace: istio-system
spec:
  compiledTemplate: metric                  <·· gets metrics from mesh and reports each
  params:                                       metric value times, in this case 2.
    value: "2"
    dimensions:
      source: source.workload.name | "unknown"
      destination: destination.workload.name | "unknown"
---
apiVersion: config.istio.io/v1alpha2
kind: handler
metadata:
  name: doublehandler
  namespace: istio-system
spec:                                                    <·· push metrics to Prometheus
  compiledAdapter: prometheus                                - It also accommodates the two new dimensions and
  params:                                                      propagates them as labels in Prometheus.
    metrics:
    - name: doublerequest_count # Prometheus metric name
      instance_name: requestdouble.instance.istio-system     <·· from
      kind: COUNTER
      label_names:
      - source
      - destination
---
apiVersion: config.istio.io/v1alpha2
kind: rule                  <··· combines deployed (fully qualified name) handlers
#                                with (fully qualified name)deployed instances
#                                (or just short-names for instances in the same  namespace)
metadata:
  name: requestdouble-prometheus
  namespace: istio-system
spec:                                                       <·· Connecting handler <··> instance
  match: match(destination.service.name, "webservice")     <·· match req. for given
  actions:                                                     condition before invoking
  - handler: doublehandler                                     the associated instances.
    instances: [ requestdouble ]
[[}]]
[[}]]# Prometheus [[{]]
* OOSS monitoring and alerting tool.
* Time Series engine with data grouped on the basis of metric name and labels (key-value pairs).
* query language to analyze the data
* An alerting system to send notifications based on analysis done in the second step

Prometheus INPUT (collects metrics):
* Pull model over HTTP.

* preconfigured to scrape Mixer endpoints to collect exposed metrics.
  - istio-telemetry.istio-system:42422/metrics: This returns all Mixer-generated metrics.
  - istio-telemetry.istio-system:15014/metrics: This returns all Mixer-specific metrics.
  - istio-proxy:15090/metrics: raw stats generated by Envoy.
  - istio-pilot.istio-system:15014/metrics:   Pilot-generated metrics
  - istio-galley.istio-system:15014/metrics   Galley-generated metrics
  - istio-policy.istio-system:15014/metrics:  all policy-related metrics
  - istio-citadel.istio-system:15014/metrics: all Citadel-generated metrics

WARN:  For a production environment, it may be better to set up Prometheus separately from istio.

* the predefined configuration has instances and handlers connected by
  rules. The predefined instances provided are:
  INSTANCE              : Captures ...
  ──────────────────────┼────────────────────────────────────────────────────
  - accesslog           : log entry for request src&dest details
  - attributes          : source and destination pod, workload, and namespace details
  - requestcount        : number of source to destination requests
  - requestduration     : response time of ALL THE CALLS in the mesh
  - requestsize         :
  - responsesize        :
  - tcpaccesslog        : metrics of the TCP request
  - tcpbytereceived     : bytes received by the destination in the TCP request
  - tcpbytesent         : bytes sent by the source in the TCP request
  - tcpconnectionsclosed: the number of times the TCP connection is closed
  - tcpconnectionsopened: the number of times TCP connection is opened

## Prometheus Custom Metrics [[{]]
* there are scenarios when the mixed metrics to Prometheus need to be recalculated to make sense.

### Eg scenario: double all the requests to the webapp service.

apiVersion: config.istio.io/v1alpha2
kind: instance                              <··· Mixer instance
metadata:
  name: requestdouble
  namespace: istio-system
spec:
  compiledTemplate: metric                  <·· gets metrics from mesh and reports each
  params:                                       metric value times, in this case 2.
    value: "2"
    dimensions:
      source: source.workload.name | "unknown"
      destination: destination.workload.name | "unknown"
---
apiVersion: config.istio.io/v1alpha2
kind: handler
metadata:
  name: doublehandler
  namespace: istio-system
spec:                                                    <·· push metrics to Prometheus
  compiledAdapter: prometheus                                - It also accommodates the two new dimensions and
  params:                                                      propagates them as labels in Prometheus.
    metrics:
    - name: doublerequest_count # Prometheus metric name
      instance_name: requestdouble.instance.istio-system     <·· from
      kind: COUNTER
      label_names:
      - source
      - destination
---
apiVersion: config.istio.io/v1alpha2
kind: rule
metadata:
  name: requestdouble-prometheus
  namespace: istio-system
spec:                                                       <·· Connecting handler <··> instance
  match: match(destination.service.name, "webservice")
  actions:
  - handler: doublehandler
    instances: [ requestdouble ]
[[}]]
[[}]]

## Grafana [[{]]
* OOSS tool to visualize, analyze, and monitor metrics.
  ```
  metrics provider -> prometheous -> Grafana -> alerts
  ```

* Preconfigured with Dashboards for Istio.
* Additional data sources & dashboards can also be added.

## Grafana Alerts

Ex alert: requests rate to webapp-deployment-v8 crosses the threshold 2.5.

1.set up the an alerting channel:
  HipChat                  Discord, Email         Pushover   Microsoft Teams
  OpsGenie                 VictorOps              Webhook    Telegram
  Sensu                    Google Hangouts Chat   DingDing
  Threema Gateway          Kafka REST Proxy       PagerDuty
  Prometheus Alertmanager  LINE                   Slack

  webhook example:
  https://myalerts.mycomp.com/api/alerts/app0023
1. Go to panel "..." ··> Alert tab ··> set up new alert

   Webhook will start receiving alerts similar to:
   {
     "evalMatches": [{
       "value": 107.19741952834556,
       "metric": "{destination=\"webapp-deployment-8\",
       instance=\"172.17.0.6:42422\", job=\"istio-mesh\",
       source=\"frontend-deployment\"}",
       "tags": {
           "destination": "webapp-deployment-8",
           "instance": "172.17.0.6:42422",
           "job": "istio-mesh",
           "source": "frontend-deployment"
       }
     }],
     "message": "Requests threshold ... reaching threshold, action required",
     "ruleId": 1,
     "ruleName": "RequestDouble Rate alert",
     "ruleUrl": "http://localhost:3000/...",
     "state": "alerting",
     "title": "[Alerting] RequestDouble Rate alert"
   }
[[}]]

## Distributed Tracing  [[{]]
   tracking a request flow across different applications.

* solutions for distributed tracing exists such as Zipkin, Jagger, Skywalking, ...
  and Istio can work with all of them.
* KEY-POINT: Distributed tracing relies on an additional set of HTTP headers.
             widely known as b3 request headers.
* In summary, the following set of headers will be propagated (by Envoy proxies)
  from an incoming request to all outgoing subrequests:
  * x-request-id
  * x-b3-traceid
  * x-b3-spanid
  * x-b3-parentspanid
  * x-b3-sampled
  * x-b3-flags

* KEYPOINT: There are language-specific OpenTracing libraries that can help to achieve
  the required header propagation.

PRESETUP:
1. deploy  jaeger (-by Uber-) Operator to k8s cluster.
  ```
  $ git clone https://github.com/jaegertracing/jaeger-operator.git
  $ kubectl create namespace observability
  namespace/observability created
  $ cd jaeger-operator/deploy
  $ kubectl create -f crds/jaegertracing_v1_jaeger_crd.yaml
  $ kubectl create -f service_account.yaml
  $ kubectl create -f role.yaml
  $ kubectl create -f role_binding.yaml
  $ kubectl create -f operator.yaml
  $ kubectl get all -n observability # check install
  ```
2. Deploy jaeger instance "AllInOne" (simplest setup) using the jaeger operator.
   configured with in-memory storage.
   "all-in-one" Pod == agent+collector+query+ingester+Jaeger UI.
   $ (
   cat << EOF
   ---
   apiVersion: jaegertracing.io/v1
   kind: Jaeger
   metadata:
     name: simplest
   EOF
   ) | kubectl apply -f jagger.yaml
   $ kubectl get all # check
   NAME
   pod/frontend-deployment-c9c975b4-p8z2t ...
   pod/simplest-56c7bd47bf-z7cnx          ...
   pod/webapp-deployment-6.2-654c5fd8f9-mrc22 ...

   NAME
   service/simplest-agent
   service/simplest-collector
   service/simplest-collector-headless
   service/simplest-query
3. Once Jagger is available, make Istio refer to it.
   1. Alt 1: While installing the service mesh:<br/>
      `global.tracer.zipkin.address=jagger-FQDN:16686`
   2. Alt 2: In existing installations:
      $ kubectl -n istio-system edit deployment istio-telemetry
4. (Optional) Instruct sidecars to start generating the traces.
   The Envoy proxy can be configured to sample a subset
   of all the received requests like:
   1. Alt 1: Set `pilot.traceSampling` var at Istio install.
   1. Alt 2: Set PILOT_TRACE_SAMPLING with:<br/>
     `$ kubectl -n istio-system edit deploy istio-pilot`
   This step is optional since maybe we are just interested
   in sending application specific traces.

   Jaeger allows to "zoom" into each trace in the global histogram of
   requests/events/... traces.
[[}]]

# mTLS Transport Authentication [[{]]
* Istio implements PKI logic with Citadel and Node-Agent.

  Cert. Emission: (repeated periodically for key and certificate rotation)
  node-agent ··> node-agent: generates private key + CSR
  node-agent ··> Citadel   : CSR
  Citadel    ··> Citadel   : validate credentials ··> sign
  Citadel    ··> node-agent: Cert
  node-agent ··> Envoy     : private key + Cert

  TLS communication:
  Envoy1 ··> Envoy2 : starts mutualTLS handshake
  Envoy1 ··> Envoy1 : secure naming check (verify Service account
                      presented in Envoy2 is is authorized to run
                      the target service).
  Envoy1<··> Envoy2 : establish mutual TLS connection

 For external services Istio provides a permissive mode allowing
 services to accept both plain-text traffic and mutual TLS traffic
 at the same time.

* mutual TLS authentication is configured by creating a policy.
  enforces the type of exchange supported by the application.

  policy scopes:
  * Service
  * Namespace
  * Mesh      : preinstalled with Istio install

  $ kubectl get meshpolicies.authentication.istio.io -o yaml
  apiVersion: v1
  items:
  - apiVersion: authentication.istio.io/v1alpha1
    kind: MeshPolicy
    metadata:
    ## REMOVED for BREVITY
      generation: 1
      labels:
        app: security
        chart: security
        heritage: Tiller
        release: istio
      name: default
    spec:
      peers:
      - mtls:
          mode: PERMISSIVE    <·· non TLS allowed from outside the mesh
  kind: List
  metadata:
    resourceVersion: ""
    selfLink: ""
  ---
  apiVersion: "authentication.istio.io/v1alpha1"
  kind: "Policy"
  metadata:
    name: "strict-policy"
  spec:
    targets:
    - name: webservice      <·· Strict policy
    peers:                      · (null defaults to strict??)
    - mtls:                     ·
        mode: null          <···┘


NOTE: As soon as mTLS  is enforced services will start to fail since live&readiness probes will fail.

FIX 1: Use probes on another port bypassing Envoy.
FIX 2: enable ProbeRewrite for checks.
       This will forward Probe requests through the Pilot-Agent
       STEP 1)
       $ kubectl get \
           cm istio-sidecar-injector \
           -n istio-system -o yaml | \
         sed -e "s/ rewriteAppHTTPProbe: false/ rewriteAppHTTPProbe: true/" | \
         kubectl apply -f -
       STEP 2) configure rewriteAppHTTPProbers annotation for our deployment.
       apiVersion: apps/v1
       kind: Deployment
       metadata:
         name: webapp-deployment-6.0
        ...
         template:
           metadata:
             ...
             annotations:
               sidecar.istio.io/rewriteAppHTTPProbers: "true"   <···
           spec:
             containers:
             - name: webapp
        ...

* instruct envoy side-car clients to perform the mtls handshake.

  apiVersion: "networking.istio.io/v1alpha3"
  kind: "DestinationRule"
  metadata:
    name: "wb-rule"
    namespace: "default"
  spec:
    host: webservice
    trafficPolicy:
      tls:
        mode: ISTIO_MUTUAL    <···
[[}]]

# User OAuth Authentication [[{]]
* Istio provides OAuth token-based authentication.
  Envoy proxy validates the token with the configured OpenID provider.
  Then the token is sent in JSON Web Token (JWT) format.

Istio authentication is performed as per the following steps:

    XXX ··> Auth_server: initial request to  exchange credentials and generate a token.
    Auth_server ··> Auth_server: new JWT with a seet of specific user roles and permissions.
    XXX ··> Envoy_proxy: JWT
    Envoy_proxy1 ··> Envoy_proxy1: Validate
    Envoy_proxy1 ··> Envoy_proxy2: JWT

PRESETUP Example:
1. Install KeyCloak OpenID provider (alternatively Auth0, Google Auth, ...)
1. select/add a realm in KeyCloak. (allowing for Apps clients identified by ID-secret pair)
1. add users in KeyCloak.
1. Configure OpenID endpoints details allowing to perform user authentication and token validation.
   issuer                   "http://172.18.0.1:8181/auth/realms/k8s-dev"
   authorization_endpoint   "http://172.18.0.1:8181/auth/realms/k8s-dev/protocol/OpenID-connect/auth"
   token_endpoint           "http://172.18.0.1:8181/auth/realms/k8s-dev/protocol/OpenID-connect/token"
   jwks_uri                 "http://172.18.0.1:8181/auth/realms/k8s-dev/protocol/OpenID-connect/certs"

1. Configure Istio user authentication by using the previously provided endpoints.

   apiVersion: "authentication.istio.io/v1alpha1"
   kind: "Policy"
   metadata:
     name: "user-auth"
   spec:
     targets:
     - name: webservice
     origins:
     - jwt:
         issuer: http://172.18.0.1:8181/auth/realms/k8s-dev
         jwksUri: http://172.18.0.1:8181/auth/realms/k8s-dev/
         protocol/OpenID  -connect/certs
         trigger_rules:
         - excluded_paths:
           - exact: /health
     principalBinding: USE_ORIGIN

   NOTE: JWT token authentication can be disabled for a specific path.
         and add multiple JWT blocks to handle different paths.
[[}]]

# RBAC Authorization [[{]]
* Envoy sidecar proxy gets the applicable RBAC policies from Pilot
  by comparing the JWT in the request against the configured
  authorization policies either allowing or denying the request.

PRESETUP)
1. enable RBAC control for "default" namespace.
   apiVersion: "rbac.istio.io/v1alpha1"
   kind: ClusterRbacConfig             <·· singleton cluster-scoped object, named default.
   metadata:
     name: default
   spec:
     mode: 'ON_WITH_INCLUSION'         <·· OFF              : authorization is disabled.
     inclusion:                            ON               : enabled for all services
       namespaces: ["default"]             ON_WITH_INCLUSION: enabled for services&namespaces specified
                                           ON_WITH_EXCLUSION: enabled for services&namespaces not specified

  RBAC can be fine-tuned for a path and/or an HTTP method and/or request headers. eg:
  ---
  apiVersion: "rbac.istio.io/v1alpha1"
  kind: ServiceRole                          <·· It needs to be assigned to a 1+ users
  metadata:                                      or left for anonymous access
    name: http-viewer
  spec:
    rules:
    - services: ["webservice"]
      methods: ["GET"]
  ---
  apiVersion: "rbac.istio.io/v1alpha1"
  kind: ServiceRole
  metadata:
    name: http-update-webservice
  spec:
    rules:
    - services: ["webservice"]
      methods: ["POST"]
  ---
  apiVersion: "rbac.istio.io/v1alpha1"
  kind: ServiceRoleBinding
  metadata:
    name: bind-http-viewer
  spec:
    subjects:
    - user:  "*"
    roleRef:
      kind: ServiceRole
      name: "http-viewer"
  ---
  apiVersion: "rbac.istio.io/v1alpha1"
  kind: ServiceRoleBinding
  metadata:
    name: bind-http-update
  spec:
    subjects:
    - properties:
        request.auth.claims[scope]: "webservice"
    roleRef:
      kind: ServiceRole
      name: "http-update-webservice"
[[}]]

# (Application) Rules [[{]]
* Eg: create rules to manage resource utilization or control
      application deny/allow listing at runtime without
      custom code.
* rule validation is provided by the Mixer.

* Reminder: Mixer consists of three parts.
  - Handler : Defines the adapter configuration
  - Instance: Defines the attributes that need to be captured
              for a request
  - Rule    : Associates a handler with instances that can send
              the required data

 Previously, the handler was capturing data in external systems.
 The handler can perform a Boolean check for the received request.

PRESETUP) Enable "disablePolicyChecks" flag to toggle a rules check

  $ kubectl get cm istio \
    -n istio-system -o yaml | \
    sed -e "s/ disablePolicyChecks: true/ disablePolicyChecks: false/" | \
    kubectl apply -f - configmap/istio configured

## configure allow-list rule:
eg: allow  acesss from front-end to webapp service.
    denied access to any other source.
  ```yaml
  ---
  apiVersion: config.istio.io/v1alpha2
  kind: handler
  metadata:
    name: allowlist01
  spec:
    compiledAdapter: listchecker
    params:
      overrides: ["frontend"]            it can be toggled to blacklisting.
      blacklist: true                <·· Istio provides also handlers to perform checks
  ---                                    for quota management, simple denials, ...
  apiVersion: config.istio.io/v1alpha2
  kind: instance
  metadata:
    name: appsource
  spec:
    compiledTemplate: listentry
    params:
      value: source.labels["app"]
  ---
  apiVersion: config.istio.io/v1alpha2
  kind: rule
  metadata:
    name: checksrc
  spec:
    match: destination.labels["app"] == "webapp"
    actions:
    - handler: allowlist01
      instances: [ appsource ]

  $ curl http://10.152.183.230/ # will fail
  $ curl $http_frontend_service # will work
  ```
[[}]]

# Troubleshooting  [[{troubleshooting]]
## istioctl analyze
```
$ istioctl analyze
✔ No validation issues found when analyzing namespace: default.
```
## k8s best-patterns for istio  [[{]]
* Istio routing requires names for k8s Service.spec.ports.name following:
  $protocol[-$suffix] format.
   ^^^^^^^^
   tcp tls udp
   http http2 https grpc
   mysql mongo redis
* Deployment.metadata.labels must contain a label "version" to allow
  Istio do routing based on application version.
* Pods exposed ports must be declared in the Deployment template
  At spec.containers[*].ports.cotnainerPort
[[}]]

## Checking Istio Configmaps  [[{]]

* Istio flags are part of the Istio configuration defined using the Kubernetes ConfigMaps.
  In particular Istio is driven by two configmaps:
  - istio                 : Configuration for Istio pilot, Mixer, Tracing, Prometheus/Grafana,...
                            (prefixed with the component name)
  - istio-sidecar-injector: Configuration for Istio sidecar, including the location of
                            the Pilot, Kubernetes api-server, etc.

  $ kubectl -n istio-system get \
      cm istio -o jsonpath="{@.data.mesh}"
  disablePolicyChecks:  false
  enableTracing: true
  accessLogFile: "/dev/stdout"
  ...

  $kubectl -n istio-system get \
     cm istio-sidecar-injector -o
  jsonpath="{@.data.config}"
  policy: enabled
  alwaysInjectSelector:
    []
  template: |-
   rewriteAppHTTPProbe: {{ valueOrDefault .Values.sidecarInjector
   Webhook.rewriteAppHTTPProbe false }}
    {{- if or (not .Values.istio_cni.enabled) .Values.global.
    proxy.enableCoreDump }}
    initContainers:
  ...
[[}]]

## Check (Envoy) sidecar automatic injection for namespaces [[{]]
   $ kubectl get namespace -L istio-injection
   NAME              STATUS   AGE    ISTIO-INJECTION
   default           Active   221d   enabled
   istio-system      Active   60d    disabled
   kube-node-lease   Active   75d
   kube-public       Active   221d
   kube-system       Active   221d

[[}]]

## Check Proxy Sync Status [[{]]
  $ istioctl proxy-status
  PROXY                                                CDS
  LDS     EDS     RDS       PILOT                          VERSION
  istio-ingress-6458b8c98f-7ks48.istio-system          SYNCED
  SYNCED  SYNCED  NOT SENT  istio-pilot-75bdf98789- n2kqh  1.1.2
  istio-ingressgateway-7d6874b48f-qxhn5.istio-system   SYNCED
  webservice-v1-55d4c455db-zjj2m.default               SYNCED
  SYNCED  SYNCED  SYNCED    istio-pilot-75bdf98789- n2kqh   1.1.2
  webservice-v2-686bbb668-99j76.default                SYNCED
  SYNCED  SYNCED  SYNCED    istio-pilot-75bdf98789- tfdvh  1.1.2
  ...
  ^^^^^^
  SYNCED  : sidecar is updated with all changes from Istio pilot
  STALE   : there are changes, but sidecar has not picked them.
  NOT SENT: There are no changes.

  If a proxy is missing in the list, it is not connected to the Istio pilot.
  We can find out a proxy configuration using the following command:

  $ istioctl proxy-config bootstrap -n istio-egressgateway-9b7866bf5-8p5rt.istio-system
  {
    "bootstrap": {
      "node": {
        "id": "router~172.30.86.14~ istio-egressgateway-...",
        "cluster": "istio-ingressgateway",
        "metadata": { ... }
        ...
      },
      ...
    }
  }

* Investigate proxy logs:
  $ kubectl logs pod/frontend-deployment-  istio-proxy
  [2019-09-15T20:17:00.310Z] "GET / HTTP/2" 204 - ... "tcp://10.0.2.1:8080"
  [2019-09-15T20:17:01.102Z] "GET / HTTP/2" 204 - ... "tcp://10.0.2.1:8080"
[[}]]


[[troubleshooting}]]


## Summary [[{]]
Istio is composed of a control plane (configuration) and data plane (data flow).
Control plane  == Mixer + Pilot + Citadel + Galley

Istio is deployed on K8s through k8s Custom Resource Definitions (CDRs).

* Request routing is done through VirtualService and DestinationRule.

* Istio Gateway Summary ("External World Interaction")

* Service Resilience Summary
  (Retries/timeouts/Load balancing/Circuit breakers/connection pools)

* Application Metrics Summary
  monitoring using Prometheus
  set up custom metrics
  glimpse into how PromQL can help in data filtering.

* Logs and Tracing Summary
  Istio allows to use the tracing solution of our choice.
  Istio does not offer a solution for application logging.
  but Kubernetes does, eg: deploying an ELK instance and
  routing application logs through Sidecar pattern.
  We worked with the Mixer component to enable sidecar logs ingestion.

* Policies and Rules Summary
Transport authentication  == mutual TLS  with auto PKI
Istio supports PERMISSIVE mode to offer plain-text interactions by default

USER AUTHENTICATION == OAuth-based JWT
RBAC AUTHORIZATION: fine-grained permissions for service/path/HTTP method
                    and request attributes.
RULE ENGINE: enforce checks such as denylist, request quotas, etc.
[[}]]

# TODO  [[{]]
## EnvoyFilters
* Envoy proxy-specific filters on top of generic
  Pilot ones. ("advanced" and "non standard", to be used with care)

[[}]]



